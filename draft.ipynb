{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "<h1 align=\"center\">CS598: Deep Learning for Healthcare Final Project</h1>\n",
    "<h3 align=\"center\">Team #164</h3>\n",
    "<h3 align=\"center\">Team member: Zexi Yan</h3>\n",
    "<h3 align=\"center\">Corresponding emails: zexiyan2@illinois.edu</h3>\n",
    "<h3 align=\"center\">Github Repo: https://github.com/ericyan3000/CS598-DLH-Chet.git</h3>\n",
    "\n",
    "\n",
    "\n",
    "# **Introduction**\n",
    "\n",
    "###   Background of the problem\n",
    "The primary challenge addressed by the paper is the limitation of existing health event prediction models which consider diagnoses as independent entities, neglecting the clinical relationships among diseases. This oversight hinders the ability to effectively utilize combinational disease information and understand the dynamic nature of disease development over time. This leads to the two problems the paper is trying to address.\n",
    "- Disease combinations in medical practice form a global graph structure that reveals hidden patterns among diseases, with individual patient visits represented as local subgraphs. Despite the potential to predict future health events by analyzing these structures, common deep learning models like GRAM [2], Timeline [3], and G-BERT [4] do not utilize this graph structure for health event predictions.\n",
    "- The progression of a disease in a patient is dynamic, as evidenced by changing diagnosis priorities and the emergence of new diagnoses in EHR datasets like MIMIC-III [5]. This dynamic nature, where diseases evolve and impact patients differently over time, suggests the need for a model that can dynamically represent disease development and learn the transition from potential to actual diagnoses.\n",
    "###   Paper explanation\n",
    "The paper titled \"Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs\" [1] introduces a novel framework for improving health event predictions by incorporating dynamic disease relationships within EHR data. The authors propose a sophisticated model that constructs and utilizes dynamic disease graphs to represent the evolving relationships between different diagnoses as patients continue to visit healthcare facilities.\n",
    "\n",
    "The innovation of the method lies in its ability to dynamically adjust disease representations and interactions based on a patient's history and current health state. This is achieved through the use of global disease co-occurrence graphs and patient-specific subgraphs, which adapt based on new health information. The model uses transition functions to model the changes in disease relevance and connections, reflecting the natural progression and regression of disease states over time. \n",
    "\n",
    "In terms of effectiveness, the proposed method has shown to outperform existing models significantly, as demonstrated through rigorous testing on real-world EHR datasets. The results indicated improvements in prediction accuracy for various health events, showcasing the model's capability to handle the complex dynamics of disease progression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# **Scope of Reproducibility:**\n",
    "\n",
    "### Model Implementation:\n",
    "We aim to implement the Chet model based on the descriptions and methodologies outlined in the original paper [1]. This includes constructing the global disease co-occurrence graph, designing dynamic subgraphs for patient visits, and defining diagnosis roles that model the transition processes of diseases. The correctness of the implementation will be verified by comparing the architectural integrity and model evaluation metrics against those described in the paper.\n",
    "\n",
    "### Experimental Reproduction:\n",
    "We will replicate the experiments conducted in the original study for the heart failure prediction task only. The evaluation metrics - weighted AUC, F1 score and test loss - will be used to assess the model performance. Our goal is to achieve results within a comparable range to those reported in the original paper.\n",
    "\n",
    "### Ablation Study\n",
    "The ablation study aims to quantify the contribution of the transition functions to the overall performance of the Chet model. By removing or modifying key components of the model, we can isolate and understand their individual impacts. For this project, we will specifically focus on the role of the transition functions, which are central to the model's ability to dynamically integrate historical context and disease progression across patient visits and diagonistic codes.\n",
    "\n",
    "Running environments, datasets, model implementation, hyperparameters and model training, albation study and analysis of results will be discussed in details in further sessions below.\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methodology**\n",
    "\n",
    "### Environment\n",
    "\n",
    "The following shows the python version and imports all required dependencies/packages for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.19 (main, Mar 21 2024, 12:07:41) \n",
      "[Clang 14.0.6 ]\n",
      "PyTorch version: 2.3.0\n",
      "NVIDIA/CUDA GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import _pickle as pickle\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# print python and package versions\n",
    "print('Python version: {}'.format(sys.version))\n",
    "print('PyTorch version: {}'.format(torch.__version__))\n",
    "print(\"NVIDIA/CUDA GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(\"Using device:\", device) # cuda, mps, cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "#### Data download\n",
    "Our reproduction will use the same public datasets mentioned in the study â€” MIMIC-III. This choice ensures that any differences in performance metrics can be attributed to the model implementation and not variations in data. <br>\n",
    "\n",
    "The data is available in https://physionet.org/content/mimiciii/1.4/. The only two datasets used in this project is `ADMISSIONS.csv` and `DIAGNOSES_ICD.csv`. These two files should be placed under the directory `data\\mimic3\\raw` for data pre-processing.\n",
    "\n",
    "#### Preprocessing code + command\n",
    "\n",
    "Data Parser<br>\n",
    "\n",
    "The data parser `EHRParser` is designed to process Electronic Health Record (EHR) data from specified file paths. It is structured to handle and parse different components of EHRs, including patient admissions and diagnostic codes. It also filtered disqualifed entries from dataset for smoother downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHRParser:\n",
    "\n",
    "    '''\n",
    "    pid_col, adm_id_col, adm_time_col, cid_col: These attributes store the column names used for \n",
    "    patient ID, admission ID, admission time, and code ID respectively, which are used across \n",
    "    various methods to access data fields in EHR datasets.\n",
    "    '''\n",
    "    pid_col = 'pid'\n",
    "    adm_id_col = 'adm_id'\n",
    "    adm_time_col = 'adm_time'\n",
    "    cid_col = 'cid'\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        self.skip_pid_check = False\n",
    "\n",
    "        self.patient_admission = None\n",
    "        self.admission_codes = None\n",
    "        self.admission_procedures = None\n",
    "        self.admission_medications = None\n",
    "\n",
    "        self.parse_fn = {'d': self.set_diagnosis}\n",
    "\n",
    "    def set_admission(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_diagnosis(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def to_standard_icd9(code: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    '''\n",
    "    Parses patient admission data from a CSV file, extracts relevant columns, and organizes admissions by patient ID into ordered dictionaries.\n",
    "    '''\n",
    "    def parse_admission(self):\n",
    "        print('parsing the csv file of admission ...')\n",
    "        filename, cols, converters = self.set_admission()\n",
    "        admissions = pd.read_csv(os.path.join(self.path, filename), usecols=list(cols.values()), converters=converters)\n",
    "        admissions = self._after_read_admission(admissions, cols)\n",
    "        all_patients = OrderedDict()\n",
    "        for i, row in admissions.iterrows():\n",
    "            if i % 100 == 0:\n",
    "                print('\\r\\t%d in %d rows' % (i + 1, len(admissions)), end='')\n",
    "            pid, adm_id, adm_time = row[cols[self.pid_col]], row[cols[self.adm_id_col]], row[cols[self.adm_time_col]]\n",
    "            if pid not in all_patients:\n",
    "                all_patients[pid] = []\n",
    "            admission = all_patients[pid]\n",
    "            admission.append({self.adm_id_col: adm_id, self.adm_time_col: adm_time})\n",
    "        print('\\r\\t%d in %d rows' % (len(admissions), len(admissions)))\n",
    "\n",
    "        patient_admission = OrderedDict()\n",
    "        for pid, admissions in all_patients.items():\n",
    "            if len(admissions) >= 2:\n",
    "                patient_admission[pid] = sorted(admissions, key=lambda admission: admission[self.adm_time_col])\n",
    "\n",
    "        self.patient_admission = patient_admission\n",
    "\n",
    "    def _after_read_admission(self, admissions, cols):\n",
    "        return admissions\n",
    "\n",
    "    '''\n",
    "    parse and process concept data related to patient healthcare records, such as diagnoses or treatments, based on a specified concept_type.\n",
    "    '''\n",
    "    def _parse_concept(self, concept_type):\n",
    "        assert concept_type in self.parse_fn.keys()\n",
    "        filename, cols, converters = self.parse_fn[concept_type]()\n",
    "        concepts = pd.read_csv(os.path.join(self.path, filename), usecols=list(cols.values()), converters=converters)\n",
    "        concepts = self._after_read_concepts(concepts, concept_type, cols)\n",
    "        result = OrderedDict()\n",
    "        for i, row in concepts.iterrows():\n",
    "            if i % 100 == 0:\n",
    "                print('\\r\\t%d in %d rows' % (i + 1, len(concepts)), end='')\n",
    "            pid = row[cols[self.pid_col]]\n",
    "            if self.skip_pid_check or pid in self.patient_admission:\n",
    "                adm_id, code = row[cols[self.adm_id_col]], row[cols[self.cid_col]]\n",
    "                if code == '':\n",
    "                    continue\n",
    "                if adm_id not in result:\n",
    "                    result[adm_id] = []\n",
    "                codes = result[adm_id]\n",
    "                codes.append(code)\n",
    "        print('\\r\\t%d in %d rows' % (len(concepts), len(concepts)))\n",
    "        return result\n",
    "\n",
    "    def _after_read_concepts(self, concepts, concept_type, cols):\n",
    "        return concepts\n",
    "\n",
    "    '''\n",
    "    parses diagnostic codes associated with admissions\n",
    "    '''\n",
    "    def parse_diagnoses(self):\n",
    "        print('parsing csv file of diagnosis ...')\n",
    "        self.admission_codes = self._parse_concept('d')\n",
    "\n",
    "    '''\n",
    "    calibrates patient records by ensuring that each patient has at least one admission record with diagnostic codes.\n",
    "    '''\n",
    "    def calibrate_patient_by_admission(self):\n",
    "        print('calibrating patients by admission ...')\n",
    "        del_pids = []\n",
    "        for pid, admissions in self.patient_admission.items():\n",
    "            for admission in admissions:\n",
    "                adm_id = admission[self.adm_id_col]\n",
    "                if adm_id not in self.admission_codes:\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            del_pids.append(pid)\n",
    "        for pid in del_pids:\n",
    "            admissions = self.patient_admission[pid]\n",
    "            for admission in admissions:\n",
    "                adm_id = admission[self.adm_id_col]\n",
    "                for concepts in [self.admission_codes]:\n",
    "                    if adm_id in concepts:\n",
    "                        del concepts[adm_id]\n",
    "            del self.patient_admission[pid]\n",
    "\n",
    "    '''\n",
    "    calibrates admission records by ensuring that each admission record has at least one diagnostic code.\n",
    "    '''\n",
    "    def calibrate_admission_by_patient(self):\n",
    "        print('calibrating admission by patients ...')\n",
    "        adm_id_set = set()\n",
    "        for admissions in self.patient_admission.values():\n",
    "            for admission in admissions:\n",
    "                adm_id_set.add(admission[self.adm_id_col])\n",
    "        del_adm_ids = [adm_id for adm_id in self.admission_codes if adm_id not in adm_id_set]\n",
    "        for adm_id in del_adm_ids:\n",
    "            del self.admission_codes[adm_id]\n",
    "\n",
    "    '''\n",
    "    samples a subset of patients from the dataset\n",
    "    '''\n",
    "    def sample_patients(self, sample_num, seed):\n",
    "        np.random.seed(seed)\n",
    "        keys = list(self.patient_admission.keys())\n",
    "        selected_pids = np.random.choice(keys, sample_num, False)\n",
    "        self.patient_admission = {pid: self.patient_admission[pid] for pid in selected_pids}\n",
    "        admission_codes = dict()\n",
    "        for admissions in self.patient_admission.values():\n",
    "            for admission in admissions:\n",
    "                adm_id = admission[self.adm_id_col]\n",
    "                admission_codes[adm_id] = self.admission_codes[adm_id]\n",
    "        self.admission_codes = admission_codes\n",
    "\n",
    "    def parse(self, sample_num=None, seed=1000):\n",
    "        self.parse_admission()\n",
    "        self.parse_diagnoses()\n",
    "        self.calibrate_patient_by_admission()\n",
    "        self.calibrate_admission_by_patient()\n",
    "        if sample_num is not None:\n",
    "            self.sample_patients(sample_num, seed)\n",
    "        return self.patient_admission, self.admission_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mimic3Parser(EHRParser):\n",
    "    def set_admission(self):\n",
    "        filename = 'ADMISSIONS.csv'\n",
    "        cols = {self.pid_col: 'SUBJECT_ID', self.adm_id_col: 'HADM_ID', self.adm_time_col: 'ADMITTIME'}\n",
    "        converter = {\n",
    "            'SUBJECT_ID': int,\n",
    "            'HADM_ID': int,\n",
    "            'ADMITTIME': lambda cell: datetime.strptime(str(cell), '%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        return filename, cols, converter\n",
    "\n",
    "    def set_diagnosis(self):\n",
    "        filename = 'DIAGNOSES_ICD.csv'\n",
    "        cols = {self.pid_col: 'SUBJECT_ID', self.adm_id_col: 'HADM_ID', self.cid_col: 'ICD9_CODE'}\n",
    "        converter = {'SUBJECT_ID': int, 'HADM_ID': int, 'ICD9_CODE': Mimic3Parser.to_standard_icd9}\n",
    "        return filename, cols, converter\n",
    "\n",
    "    @staticmethod\n",
    "    def to_standard_icd9(code: str):\n",
    "        code = str(code)\n",
    "        if code == '':\n",
    "            return code\n",
    "        split_pos = 4 if code.startswith('E') else 3\n",
    "        icd9_code = code[:split_pos] + '.' + code[split_pos:] if len(code) > split_pos else code\n",
    "        return icd9_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Configuration and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for dataset processing specific to the Mimic3 dataset\n",
    "conf = {'mimic3': {\n",
    "        'parser': Mimic3Parser,\n",
    "        'train_num': 6000,\n",
    "        'test_num': 1000,\n",
    "        'threshold': 0.01,\n",
    "        'seed': 1234\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flag to determine if data should be loaded from previously saved files\n",
    "from_saved = True\n",
    "data_path = 'data'\n",
    "dataset = 'mimic3' \n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "raw_path = os.path.join(dataset_path, 'raw')\n",
    "\n",
    "# Check if raw data directory exists, create if not and prompt for data placement\n",
    "if not os.path.exists(raw_path):\n",
    "    os.makedirs(raw_path)\n",
    "    print('please put the CSV files in `data/%s/raw`' % dataset)\n",
    "    exit()\n",
    "\n",
    "# Path for storing parsed data\n",
    "parsed_path = os.path.join(dataset_path, 'parsed')\n",
    "\n",
    "# Load or parse data depending on `from_saved` flag\n",
    "if from_saved:\n",
    "    # Load previously saved parsed data\n",
    "    patient_admission = pickle.load(open(os.path.join(parsed_path, 'patient_admission.pkl'), 'rb'))\n",
    "    admission_codes = pickle.load(open(os.path.join(parsed_path, 'admission_codes.pkl'), 'rb'))\n",
    "else:\n",
    "    # Parse new data from raw files\n",
    "    parser = conf[dataset]['parser'](raw_path)\n",
    "    sample_num = conf[dataset].get('sample_num', None)\n",
    "    seed = conf[dataset]['seed']\n",
    "    patient_admission, admission_codes = parser.parse(sample_num, seed)\n",
    "    print('saving parsed data ...')\n",
    "    if not os.path.exists(parsed_path):\n",
    "        os.makedirs(parsed_path)\n",
    "    pickle.dump(patient_admission, open(os.path.join(parsed_path, 'patient_admission.pkl'), 'wb'))\n",
    "    pickle.dump(admission_codes, open(os.path.join(parsed_path, 'admission_codes.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics of admission and diagnostic codes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient num: 7493\n",
      "max admission num: 42\n",
      "mean admission num: 2.66\n",
      "max code num in an admission: 39\n",
      "mean code num in an admission: 13.06\n"
     ]
    }
   ],
   "source": [
    "# Calculate various statistics from the patient admissions data\n",
    "patient_num = len(patient_admission)\n",
    "max_admission_num = max([len(admissions) for admissions in patient_admission.values()])\n",
    "avg_admission_num = sum([len(admissions) for admissions in patient_admission.values()]) / patient_num\n",
    "max_visit_code_num = max([len(codes) for codes in admission_codes.values()])\n",
    "avg_visit_code_num = sum([len(codes) for codes in admission_codes.values()]) / len(admission_codes)\n",
    "print('patient num: %d' % patient_num)\n",
    "print('max admission num: %d' % max_admission_num)\n",
    "print('mean admission num: %.2f' % avg_admission_num)\n",
    "print('max code num in an admission: %d' % max_visit_code_num)\n",
    "print('mean code num in an admission: %.2f' % avg_visit_code_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding of medical codes, generation of hierarchical levels for these codes, and the serialization of the results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding code ...\n",
      "There are 4880 codes\n",
      "generating code levels ...\n"
     ]
    }
   ],
   "source": [
    "from preprocess.auxiliary import parse_icd9_range\n",
    "\n",
    "def encode_code(patient_admission, admission_codes):\n",
    "    \n",
    "    '''\n",
    "    Map each unique medical code to a unique integer. \n",
    "    '''\n",
    "    code_map = OrderedDict()\n",
    "    for pid, admissions in patient_admission.items():\n",
    "        for admission in admissions:\n",
    "            codes = admission_codes[admission[EHRParser.adm_id_col]]\n",
    "            for code in codes:\n",
    "                if code not in code_map:\n",
    "                    code_map[code] = len(code_map)\n",
    "\n",
    "    '''\n",
    "    Encode medical codes for each admission using the integer mapping.\n",
    "    '''\n",
    "    admission_codes_encoded = {\n",
    "        admission_id: list(set(code_map[code] for code in codes))\n",
    "        for admission_id, codes in admission_codes.items()\n",
    "    }\n",
    "    return admission_codes_encoded, code_map\n",
    "\n",
    "'''\n",
    "processes a given mapping of medical codes to numerical identifiers (code_map) and \n",
    "generates a matrix that assigns a hierarchical level structure to each code. \n",
    "'''\n",
    "def generate_code_levels(path, code_map: dict) -> np.ndarray:\n",
    "    print('generating code levels ...')\n",
    "    import os\n",
    "    three_level_code_set = set(code.split('.')[0] for code in code_map)\n",
    "    icd9_path = os.path.join(path, 'icd9.txt')\n",
    "    icd9_range = list(open(icd9_path, 'r', encoding='utf-8').readlines())\n",
    "    three_level_dict = dict()\n",
    "    level1, level2, level3 = (0, 0, 0)\n",
    "    level1_can_add = False\n",
    "    for range_ in icd9_range:\n",
    "        range_ = range_.rstrip()\n",
    "        if range_[0] == ' ':\n",
    "            prefix, format_, start, end = parse_icd9_range(range_)\n",
    "            level2_cannot_add = True\n",
    "            for i in range(start, end + 1):\n",
    "                code = prefix + format_ % i\n",
    "                if code in three_level_code_set:\n",
    "                    three_level_dict[code] = [level1, level2, level3]\n",
    "                    level3 += 1\n",
    "                    level1_can_add = True\n",
    "                    level2_cannot_add = False\n",
    "            if not level2_cannot_add:\n",
    "                level2 += 1\n",
    "        else:\n",
    "            if level1_can_add:\n",
    "                level1 += 1\n",
    "                level1_can_add = False\n",
    "\n",
    "    code_level = dict()\n",
    "    for code, cid in code_map.items():\n",
    "        three_level_code = code.split('.')[0]\n",
    "        three_level = three_level_dict[three_level_code]\n",
    "        code_level[code] = three_level + [cid]\n",
    "\n",
    "    code_level_matrix = np.zeros((len(code_map), 4), dtype=int)\n",
    "    for code, cid in code_map.items():\n",
    "        code_level_matrix[cid] = code_level[code]\n",
    "\n",
    "    return code_level_matrix\n",
    "\n",
    "# Encode diagnosis codes and generate a code map\n",
    "print('encoding code ...')\n",
    "admission_codes_encoded, code_map = encode_code(patient_admission, admission_codes)\n",
    "code_num = len(code_map)\n",
    "print('There are %d codes' % code_num)\n",
    "\n",
    "# Generate levels for each code and save\n",
    "code_levels = generate_code_levels(data_path, code_map)\n",
    "pickle.dump({\n",
    "    'code_levels': code_levels,\n",
    "}, open(os.path.join(parsed_path, 'code_levels.pkl'), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into `train`, `validation` and `test` group with approximate 8:1:1 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num: 6000, test_num: 750\n",
      "common_pids: 2281\n",
      "remaining_pids: 5212\n",
      "There are 6000 train, 743 valid, 750 test samples\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Patients who have admissions containing any of the codes are added to common_pids. This step ensures \n",
    "that at least one admission from these patients will be included in the training dataset, as these \n",
    "patients are considered to have significant or relevant data.\n",
    "2. To further ensure the diversity and information richness of the training set, the function identifies \n",
    "the patient with the maximum number of admissions and adds this patient to common_pids.\n",
    "'''\n",
    "\n",
    "def split_patients(patient_admission, admission_codes, code_map, train_num, test_num, seed=1000):\n",
    "    print(\"train_num: %d, test_num: %d\" % (train_num, test_num))\n",
    "    np.random.seed(seed)\n",
    "    common_pids = set()\n",
    "    for i, code in enumerate(code_map):\n",
    "        print('\\r\\t%.2f%%' % ((i + 1) * 100 / len(code_map)), end='')\n",
    "        for pid, admissions in patient_admission.items():\n",
    "            for admission in admissions:\n",
    "                codes = admission_codes[admission[EHRParser.adm_id_col]]\n",
    "                if code in codes:\n",
    "                    common_pids.add(pid)\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "    #print('\\r\\t100%')\n",
    "    max_admission_num = 0\n",
    "    pid_max_admission_num = 0\n",
    "    for pid, admissions in patient_admission.items():\n",
    "        if len(admissions) > max_admission_num:\n",
    "            max_admission_num = len(admissions)\n",
    "            pid_max_admission_num = pid\n",
    "    common_pids.add(pid_max_admission_num)\n",
    "    print(\"\\rcommon_pids: %d\" % len(common_pids))\n",
    "    remaining_pids = np.array(list(set(patient_admission.keys()).difference(common_pids)))\n",
    "    print(\"remaining_pids: %d\" % len(remaining_pids))\n",
    "    np.random.shuffle(remaining_pids)\n",
    "\n",
    "    valid_num = len(patient_admission) - train_num - test_num\n",
    "    train_pids = np.array(list(common_pids.union(set(remaining_pids[:(train_num - len(common_pids))].tolist()))))\n",
    "    valid_pids = remaining_pids[(train_num - len(common_pids)):(train_num + valid_num - len(common_pids))]\n",
    "    test_pids = remaining_pids[(train_num + valid_num - len(common_pids)):]\n",
    "    return train_pids, valid_pids, test_pids\n",
    "\n",
    "# Split patients into training, validation, and test sets\n",
    "train_pids, valid_pids, test_pids = split_patients(\n",
    "    patient_admission=patient_admission,\n",
    "    admission_codes=admission_codes,\n",
    "    code_map=code_map,\n",
    "    train_num=conf[dataset]['train_num'],\n",
    "    test_num=conf[dataset]['test_num']\n",
    ")\n",
    "print('There are %d train, %d valid, %d test samples' % (len(train_pids), len(valid_pids), len(test_pids)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an adjacency matrix for medical codes based on their co-occurrence within patient records in a training dataset. This matrix will serve as a graphical representation of the relationships between different medical codes, where each code represents a node in the graph, and connections (edges) between nodes indicate that the corresponding medical codes have appeared together within one or more patient admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating code code adjacent matrix ...\n",
      "\t6000 / 6000\n"
     ]
    }
   ],
   "source": [
    "from preprocess.auxiliary import normalize_adj\n",
    "\n",
    "def generate_code_code_adjacent(pids, patient_admission, admission_codes_encoded, code_num, threshold=0.01):\n",
    "    print('generating code code adjacent matrix ...')\n",
    "    n = code_num\n",
    "    adj = np.zeros((n, n), dtype=int)\n",
    "    for i, pid in enumerate(pids):\n",
    "        print('\\r\\t%d / %d' % (i, len(pids)), end='')\n",
    "        for admission in patient_admission[pid]:\n",
    "            codes = admission_codes_encoded[admission[EHRParser.adm_id_col]]\n",
    "            for row in range(len(codes) - 1):\n",
    "                for col in range(row + 1, len(codes)):\n",
    "                    c_i = codes[row]\n",
    "                    c_j = codes[col]\n",
    "                    adj[c_i, c_j] += 1\n",
    "                    adj[c_j, c_i] += 1\n",
    "    print('\\r\\t%d / %d' % (len(pids), len(pids)))\n",
    "    norm_adj = normalize_adj(adj)\n",
    "    a = norm_adj < threshold\n",
    "    b = adj.sum(axis=-1, keepdims=True) > (1 / threshold)\n",
    "    adj[np.logical_and(a, b)] = 0\n",
    "    return adj\n",
    "\n",
    "code_adj = generate_code_code_adjacent(pids=train_pids, patient_admission=patient_admission,\n",
    "                                        admission_codes_encoded=admission_codes_encoded,\n",
    "                                        code_num=code_num, threshold=conf[dataset]['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train codes features and labels ...\n",
      "\t6000 / 6000\n",
      "building valid codes features and labels ...\n",
      "\t743 / 743\n",
      "building test codes features and labels ...\n",
      "\t750 / 750\n",
      "generating train neighbors ...\n",
      "\t6000 / 6000\n",
      "generating valid neighbors ...\n",
      "\t743 / 743\n",
      "generating test neighbors ...\n",
      "\t750 / 750\n"
     ]
    }
   ],
   "source": [
    "from preprocess.build_dataset import build_code_xy\n",
    "from preprocess.auxiliary import generate_neighbors\n",
    "\n",
    "# Additional data processing for training, validation, and test sets\n",
    "common_args = [patient_admission, admission_codes_encoded, max_admission_num, code_num]\n",
    "print('building train codes features and labels ...')\n",
    "(train_code_x, train_codes_y, train_visit_lens) = build_code_xy(train_pids, *common_args)\n",
    "print('building valid codes features and labels ...')\n",
    "(valid_code_x, valid_codes_y, valid_visit_lens) = build_code_xy(valid_pids, *common_args)\n",
    "print('building test codes features and labels ...')\n",
    "(test_code_x, test_codes_y, test_visit_lens) = build_code_xy(test_pids, *common_args)\n",
    "\n",
    "# Generate neighbor nodes for training, validation, and test sets\n",
    "print('generating train neighbors ...')\n",
    "train_neighbors = generate_neighbors(train_code_x, train_visit_lens, code_adj)\n",
    "print('generating valid neighbors ...')\n",
    "valid_neighbors = generate_neighbors(valid_code_x, valid_visit_lens, code_adj)\n",
    "print('generating test neighbors ...')\n",
    "test_neighbors = generate_neighbors(test_code_x, test_visit_lens, code_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below `divide_middle` function plays a critical role within the framework described in the paper [1], which emploies constructing a global disease co-occurrence graph and designing dynamic subgraphs for each patient visit. The utility of this function categorizes diseases (or medical codes) into three distinct roles during each patient visit, reflecting the conceptual model of disease transitions. These roles are `diagnosis`, `diagnosis-neighbor`, `neighbors`. Together they construct the `dyanmic local subgraphs` as in the following image [1]\n",
    "\n",
    "<p align=\"center\"> <img src=\"images\\image_1.jpg\" alt=\"Image 1\"> </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating train middles ...\n",
      "\t6000 / 6000\n",
      "generating valid middles ...\n",
      "\t743 / 743\n",
      "generating test middles ...\n",
      "\t750 / 750\n"
     ]
    }
   ],
   "source": [
    "def divide_middle(code_x, neighbors, lens):\n",
    "    n = len(code_x)\n",
    "    divided = np.zeros((*code_x.shape, 3), dtype=bool)\n",
    "    for i, admissions in enumerate(code_x):\n",
    "        print('\\r\\t%d / %d' % (i + 1, n), end='')\n",
    "        divided[i, 0, :, 0] = admissions[0]\n",
    "        for j in range(1, lens[i]):\n",
    "            codes_set = set(np.where(admissions[j] == 1)[0])\n",
    "            m_set = set(np.where(admissions[j - 1] == 1)[0])\n",
    "            n_set = set(np.where(neighbors[i][j - 1] == 1)[0])\n",
    "            m1 = codes_set.intersection(m_set)\n",
    "            m2 = codes_set.intersection(n_set)\n",
    "            m3 = codes_set.difference(m_set).difference(n_set)\n",
    "            if len(m1) > 0:\n",
    "                divided[i, j, np.array(list(m1)), 0] = 1\n",
    "            if len(m2) > 0:\n",
    "                divided[i, j, np.array(list(m2)), 1] = 1\n",
    "            if len(m3) > 0:\n",
    "                divided[i, j, np.array(list(m3)), 2] = 1\n",
    "    print('\\r\\t%d / %d' % (n, n))\n",
    "    return divided\n",
    "\n",
    "print('generating train middles ...')\n",
    "train_divided = divide_middle(train_code_x, train_neighbors, train_visit_lens)\n",
    "print('generating valid middles ...')\n",
    "valid_divided = divide_middle(valid_code_x, valid_neighbors, valid_visit_lens)\n",
    "print('generating test middles ...')\n",
    "test_divided = divide_middle(test_code_x, test_neighbors, test_visit_lens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate labels for a dataset specifically identifying cases of heart failure based on medical code `428`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train heart failure labels ...\n",
      "building valid heart failure labels ...\n",
      "building test heart failure labels ...\n"
     ]
    }
   ],
   "source": [
    "from preprocess.build_dataset import build_heart_failure_y\n",
    "\n",
    "print('building train heart failure labels ...')\n",
    "train_hf_y = build_heart_failure_y('428', train_codes_y, code_map)\n",
    "print('building valid heart failure labels ...')\n",
    "valid_hf_y = build_heart_failure_y('428', valid_codes_y, code_map)\n",
    "print('building test heart failure labels ...')\n",
    "test_hf_y = build_heart_failure_y('428', test_codes_y, code_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show statistics for heart failure datasets. The counts of heart failure are about 1:3 in all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2121 patients have heart failure out of 6000 patients\n",
      "259 patients have heart failure out of 743 patients\n",
      "276 patients have heart failure out of 750 patients\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from preprocess.auxiliary import show_heart_failure_stat\n",
    "\n",
    "train_hf_count, train_total = show_heart_failure_stat('428', train_codes_y, code_map)\n",
    "valid_hf_count, valid_total = show_heart_failure_stat('428', valid_codes_y, code_map)\n",
    "test_hf_count, test_total = show_heart_failure_stat('428', test_codes_y, code_map)\n",
    "\n",
    "\n",
    "# Data\n",
    "labels = ['Train', 'Validation', 'Test']\n",
    "hf_counts = [train_hf_count, valid_hf_count, test_hf_count]\n",
    "total_counts = [train_total, valid_total, test_total]\n",
    "\n",
    "# Plotting\n",
    "x = range(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x, total_counts, width, label='Total Patients')\n",
    "rects2 = ax.bar(x, hf_counts, width, label='Patients with Heart Failure', bottom=total_counts - np.array(hf_counts))\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_title('Heart Failure Statistics by Dataset')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**\n",
    "\n",
    "### Original Paper \n",
    "Lu, Chang, Tian Han, and Yue Ning. \"Context-aware health event prediction via transition functions on dynamic disease graphs.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 4, pp. 4567-4574. 2022. <br>\n",
    "[Original Paper Link](https://arxiv.org/abs/2112.05195) <br>\n",
    "[Original Paper Repo](https://github.com/LuChang-CS/Chet)\n",
    "\n",
    "### Model Description\n",
    "The Chet model effectively incorporates both global and local contexts by constructing a global disease co-occurrence graph, which captures broad relationships among various diseases across different patients and healthcare settings. Additionally, for each patient visit, Chet designs dynamic subgraphs tailored to the specific medical history and the conditions of the patient.\n",
    "\n",
    "### Key Components of the Chet Model:\n",
    "1. Global Disease Co-occurrence Graph:\n",
    "   - This graph encapsulates the relationships among various diseases based on their co-occurrence in a wide range of patients. It provides a macro-level view of how diseases interact, which aids in understanding common pathways and potential complications across different demographic and medical backgrounds.\n",
    "2. Dynamic Subgraphs for Each Visit:\n",
    "   - The model generates specific subgraphs for each patient visit, utilizing the data from the global graph and adapting it to the current state and medical history of the patient. This method ensures that the predictions and insights are personalized and relevant to each individual's healthcare trajectory.\n",
    "3. Diagnosis Roles in Visits:\n",
    "   - Chet categorizes diagnoses during each visit into three roles based on the evolution of node properties in the disease graph:\n",
    "     - Continued Diagnoses: Diseases that persist from previous visits.\n",
    "     - Influenced Diagnoses: New diagnoses that are influenced by the patient's existing conditions or neighboring diagnoses in the graph.\n",
    "     - New Diagnoses: Newly emerging conditions that do not have a direct link to previous diagnoses.\n",
    "4. Transition Functions:\n",
    "   - These functions are central to Chet, as they dynamically model how a patientâ€™s disease state evolves from one visit to another. This involves updating the node properties in the subgraphs based on the observed transitions in the patientâ€™s conditions, effectively capturing how one health event leads to another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation code\n",
    "\n",
    "\n",
    "The `SingleHeadAttentionLayer` and `DotProductAttention` classes are both designed to compute attention scores for tasks where the relationships between different inputs need to be dynamically weighted. The attention mechanisms help in capturing the relative importance of different elements within a data set, such as medical codes or visit histories.\n",
    "\n",
    "- `SingleHeadAttentionLayer` is used to dynamically weigh the importance of different diagnoses or symptoms reported during a patient's visit based on other contextual information (like previous visits or related disease).\n",
    "  \n",
    "- `DotProductAttention` is used to focus on specific aspects of a patient's record in relation to a common medical context to help identifying features that are particularly significant given the patient's overall health profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "# Define a SingleHeadAttentionLayer class that extends nn.Module.\n",
    "class SingleHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, attention_size):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        self.attention_size = attention_size  # Store the attention size\n",
    "        \n",
    "        # Define Linear transformations for query, key, and value vectors\n",
    "        self.dense_q = nn.Linear(query_size, attention_size)  # Transforms input query to the attention space\n",
    "        self.dense_k = nn.Linear(key_size, attention_size)    # Transforms input key to the attention space\n",
    "        self.dense_v = nn.Linear(query_size, value_size)      # Transforms input value to the value space\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # Apply linear transformations\n",
    "        query = self.dense_q(q)  # Transform query vector\n",
    "        key = self.dense_k(k)    # Transform key vector\n",
    "        value = self.dense_v(v)  # Transform value vector\n",
    "        \n",
    "        # Compute the attention scores\n",
    "        # Scaled dot product attention mechanism\n",
    "        g = torch.div(torch.matmul(query, key.T), math.sqrt(self.attention_size))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        score = torch.softmax(g, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of values based on the attention scores\n",
    "        output = torch.sum(torch.unsqueeze(score, dim=-1) * value, dim=-2)\n",
    "        return output\n",
    "\n",
    "# Define a DotProductAttention class that also extends nn.Module.\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, value_size, attention_size):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        self.attention_size = attention_size  # Store the attention size\n",
    "        # Initialize a context vector as a learnable parameter\n",
    "        self.context = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(attention_size, 1)))\n",
    "        self.dense = nn.Linear(value_size, attention_size)  # Transforms input value to the attention space\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transform input x to attention space\n",
    "        t = self.dense(x)\n",
    "        \n",
    "        # Compute unnormalized attention scores by projecting 't' onto 'context'\n",
    "        vu = torch.matmul(t, self.context).squeeze()\n",
    "        \n",
    "        # Apply softmax to get normalized attention weights\n",
    "        score = torch.softmax(vu, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of the original inputs based on the attention weights\n",
    "        output = torch.sum(x * torch.unsqueeze(score, dim=-1), dim=-2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmbeddingLayer\n",
    "\n",
    "The `EmbeddingLayer` class in the context of the Chet model serves as a foundational component for transforming discrete medical codes into dense vector representations. These embeddings are crucial for subsequent processing layers, such as the GraphLayer, where the relationships between different diseases (represented by codes) are analyzed within a graph-based framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmbeddingLayer: Handles the embedding of codes into vectors.\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, code_num, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        # Number of unique codes\n",
    "        self.code_num = code_num\n",
    "        # Embedding parameters initialized using Xavier uniform distribution\n",
    "        # c_embeddings for center node embeddings\n",
    "        self.c_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
    "        # n_embeddings for neighbor node embeddings\n",
    "        self.n_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
    "        # u_embeddings for other uses, potentially for graph-level embeddings\n",
    "        self.u_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, graph_size)))\n",
    "\n",
    "    def forward(self):\n",
    "        # Return all embeddings as outputs\n",
    "        return self.c_embeddings, self.n_embeddings, self.u_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphLayer\n",
    "The `GraphLayer` class in the Chet model is a critical component designed to process and transform disease embeddings using the structure provided by an adjacency matrix. This layer is key to integrating the context of disease relationships into the model's understanding and predictions. It essentially serves as the mechanism through which the embeddings of diseases (or medical codes) are influenced by their relationships to other diseases, as dictated by the global disease co-occurrence graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphLayer: Processes embeddings via graph structure using adjacency matrix.\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, adj, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        self.adj = torch.tensor(adj, dtype=torch.float32).to(device)  # Convert numpy array to tensor\n",
    "        self.dense = nn.Linear(code_size, graph_size)  # Dense layer for transforming embeddings\n",
    "        self.activation = nn.LeakyReLU()  # Activation function to introduce non-linearity\n",
    "\n",
    "    def forward(self, code_x, neighbor, c_embeddings, n_embeddings):\n",
    "        \n",
    "        # Move embeddings to device (GPU or CPU)\n",
    "        c_embeddings = c_embeddings.to(device)\n",
    "        n_embeddings = n_embeddings.to(device)\n",
    "        \n",
    "        # Apply unsqueeze to expand dimensions for matrix operations\n",
    "        center_codes = torch.unsqueeze(code_x, dim=-1).to(device)\n",
    "        neighbor_codes = torch.unsqueeze(neighbor, dim=-1).to(device)\n",
    "\n",
    "        # Compute embeddings based on input codes and their neighbors\n",
    "        center_embeddings = center_codes * c_embeddings\n",
    "        neighbor_embeddings = neighbor_codes * n_embeddings\n",
    "        \n",
    "        # Multiply embeddings by adjacency matrix to propagate through the graph\n",
    "        cc_embeddings = center_codes * torch.matmul(self.adj, center_embeddings)\n",
    "        cn_embeddings = center_codes * torch.matmul(self.adj, neighbor_embeddings)\n",
    "        nn_embeddings = neighbor_codes * torch.matmul(self.adj, neighbor_embeddings)\n",
    "        nc_embeddings = neighbor_codes * torch.matmul(self.adj, center_embeddings)\n",
    "\n",
    "        # Combine embeddings and pass through dense layer with activation\n",
    "        co_embeddings = self.activation(self.dense(center_embeddings + cc_embeddings + cn_embeddings))\n",
    "        no_embeddings = self.activation(self.dense(neighbor_embeddings + nn_embeddings + nc_embeddings))\n",
    "        return co_embeddings, no_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransitionLayer\n",
    "\n",
    "The `TransitionLayer` within the Chet model is a sophisticated neural network component designed to manage state transitions across patient visits, leveraging both the gated recurrent unit (GRU) mechanism and attention mechanisms to process changes in disease states over time. This layer is integral for understanding how patient conditions evolve, providing critical insights for predicting future health events based on past data.\n",
    "\n",
    "Iit directly updates the hidden state for codes that continue from the previous time step using the GRU, focusing on maintaining historical continuity (`m1`). When applicable, it combines and processes embeddings through the attention mechanism for codes that are either influenced by neighbors or are unrelated to previous states (`m2` and `m3`). This step occurs only if the time step (t) is greater than 0, indicating subsequent visits in a patient's history.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransitionLayer: Manages state transitions in graph sequences using GRU and attention.\n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, code_num, graph_size, hidden_size, t_attention_size, t_output_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(input_size=graph_size, hidden_size=hidden_size)  # GRU cell for state transitions\n",
    "        # Attention layer for processing graph-level information\n",
    "        self.single_head_attention = SingleHeadAttentionLayer(graph_size, graph_size, t_output_size, t_attention_size)\n",
    "        self.activation = nn.Tanh()  # Tanh activation for smooth non-linearity\n",
    "\n",
    "        self.code_num = code_num  # Total number of codes\n",
    "        self.hidden_size = hidden_size  # Dimension of hidden state\n",
    "\n",
    "    def forward(self, t, co_embeddings, divided, no_embeddings, unrelated_embeddings, hidden_state=None):\n",
    "        # Process middle states based on input divisions\n",
    "        m1, m2, m3 = divided[:, 0], divided[:, 1], divided[:, 2]\n",
    "        # Find indices where division values are positive\n",
    "        m1_index = torch.where(m1 > 0)[0]\n",
    "        m2_index = torch.where(m2 > 0)[0]\n",
    "        m3_index = torch.where(m3 > 0)[0]\n",
    "        # Initialize new hidden state for all codes\n",
    "        h_new = torch.zeros((self.code_num, self.hidden_size), dtype=co_embeddings.dtype).to(co_embeddings.device)\n",
    "        output_m1 = 0\n",
    "        output_m23 = 0\n",
    "        # Compute new state for m1 divisions\n",
    "        if len(m1_index) > 0:\n",
    "            m1_embedding = co_embeddings[m1_index]\n",
    "            h = hidden_state[m1_index] if hidden_state is not None else None\n",
    "            h_m1 = self.gru(m1_embedding, h)\n",
    "            h_new[m1_index] = h_m1\n",
    "            output_m1, _ = torch.max(h_m1, dim=-2)\n",
    "        # Compute new state for m2 and m3 divisions if t > 0\n",
    "        if t > 0 and len(m2_index) + len(m3_index) > 0:\n",
    "            # Combine embeddings for m2 and m3 indices for attention processing\n",
    "            q = torch.vstack([no_embeddings[m2_index], unrelated_embeddings[m3_index]])\n",
    "            v = torch.vstack([co_embeddings[m2_index], co_embeddings[m3_index]])\n",
    "            \n",
    "            # Process combined embeddings through the attention layer\n",
    "            h_m23 = self.activation(self.single_head_attention(q, q, v))\n",
    "            \n",
    "            # Update the hidden states for m2 and m3 indices based on attention outputs\n",
    "            h_new[m2_index] = h_m23[:len(m2_index)]\n",
    "            h_new[m3_index] = h_m23[len(m2_index):]\n",
    "            \n",
    "            # Determine the maximum output across m2 and m3 for use in the final output\n",
    "            output_m23, _ = torch.max(h_m23, dim=-2)\n",
    "        \n",
    "        # Determine the final output based on available indices\n",
    "        if len(m1_index) == 0:\n",
    "            output = output_m23\n",
    "        elif len(m2_index) + len(m3_index) == 0:\n",
    "            output = output_m1\n",
    "        else:\n",
    "            # If both m1 and m2/m3 indices have outputs, take the maximum across both\n",
    "            output, _ = torch.max(torch.vstack([output_m1, output_m23]), dim=-2)\n",
    "\n",
    "        # Return the final aggregated output and the updated hidden states\n",
    "        return output, h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifer\n",
    "The `Classifier` class in the Chet model functions as a straightforward neural network module designed specifically for classification tasks. This component is pivotal for translating the sophisticated features and embeddings generated by the model into actionable predictions, such as the likelihood of specific health events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: A simple neural network module for classification tasks.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0., activation=None):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)  # Linear transformation to the output size\n",
    "        self.activation = activation  # Optional activation function\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Dropout layer to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.dropout(x)  # Apply dropout to the input\n",
    "        output = self.linear(output)  # Apply linear transformation\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)  # Apply activation function if provided\n",
    "        return output  # Return the final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chet Model\n",
    "\n",
    "Here is a general flow of the process within the Chet model, illustrating how various components work together to analyze and predict health-related outcomes:\n",
    "\n",
    "1. Embedding Generation\n",
    "   - Start Point: The process begins with the EmbeddingLayer, which takes discrete medical codes as input.\n",
    "   - Function: This layer converts each medical code into a set of dense vector embeddings. There are three types of embeddings generated: center node embeddings, neighbor node embeddings, and other potential use embeddings, facilitating diverse input types for subsequent layers.\n",
    "2. Graph-Based Processing\n",
    "   - Node Interaction: Following embedding, the GraphLayer processes these embeddings through the structure defined by an adjacency matrix. This matrix encapsulates the relationships between different medical conditions, indicating how one condition may influence another.\n",
    "   - Embedding Update: The layer updates embeddings based on these relationships, thus modeling how diseases interact within the graph context.\n",
    "3. Transition Handling\n",
    "   - Dynamic Updates: The TransitionLayer then manages the temporal dynamics of these interactions. It updates the state of graph nodes (conditions) across sequential patient visits, considering both the properties of the nodes and the transitions influenced by connected nodes.\n",
    "   - State Evolution: This layer tracks and updates the health state of a patient over time, using mechanisms like GRU cells for maintaining historical information and processing temporal transitions effectively.\n",
    "4. Attention Mechanism\n",
    "   - Sequence Aggregation: After processing the sequences through the transition layer, the DotProductAttention focuses on aggregating the sequence outputs. It highlights the most relevant features for prediction by assessing the importance of different aspects of the sequence.\n",
    "   - Feature Highlighting: This step ensures that the most informative parts of the patient's visit sequences are emphasized, enhancing the input to the final classification stage.\n",
    "5. Classification and Prediction\n",
    "   - Outcome Prediction: The final component is the Classifier, which takes the aggregated and attention-focused features to predict outcomes. This could be the likelihood of specific health events or other relevant medical predictions.\n",
    "   - Activation and Dropout: Utilizing an optional activation function and a dropout layer, this step helps in generalizing the model better to new, unseen data by preventing overfitting.\n",
    "6. Output\n",
    "   - Final Results: The model outputs predictions for each patient visit sequence processed, providing insights into potential future health events based on the learned patterns from historical and current medical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Main model integrating various components for processing graphs.\n",
    "class Chet(nn.Module):\n",
    "    def __init__(self, code_num, code_size, adj, graph_size, hidden_size,\n",
    "                 t_attention_size, t_output_size, output_size, dropout_rate, activation):\n",
    "        super().__init__()\n",
    "        # Initialize embedding, graph, and transition layers\n",
    "        self.embedding_layer = EmbeddingLayer(code_num, code_size, graph_size)\n",
    "        self.graph_layer = GraphLayer(adj, code_size, graph_size)\n",
    "        self.transition_layer = TransitionLayer(code_num, graph_size, hidden_size, t_attention_size, t_output_size)\n",
    "        self.attention = DotProductAttention(hidden_size, 32)  # Dot product attention mechanism\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate, activation)  # Classifier component\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        # Generate embeddings from the embedding layer\n",
    "        embeddings = self.embedding_layer()\n",
    "        c_embeddings, n_embeddings, u_embeddings = embeddings\n",
    "        output = []\n",
    "        # Process each sequence in the batch\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            no_embeddings_i_prev = None  # Store previous neighbor embeddings\n",
    "            output_i = []\n",
    "            h_t = None  # Initialize hidden state\n",
    "            # Iterate over time steps within a sequence\n",
    "            for t, (c_it, d_it, n_it, len_it) in enumerate(zip(code_x_i, divided_i, neighbor_i, range(len_i))):\n",
    "                # Process current time step using the graph layer\n",
    "                co_embeddings, no_embeddings = self.graph_layer(c_it, n_it, c_embeddings, n_embeddings)\n",
    "                # Transition layer updates based on current and previous states\n",
    "                output_it, h_t = self.transition_layer(t, co_embeddings, d_it, no_embeddings_i_prev, u_embeddings, h_t)\n",
    "                no_embeddings_i_prev = no_embeddings  # Update previous embeddings\n",
    "                output_i.append(output_it)  # Collect outputs for each time step\n",
    "            # Apply attention to the sequence of outputs\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)  # Collect final outputs for all sequences\n",
    "\n",
    "        output = torch.vstack(output)  # Stack all sequence outputs\n",
    "        output = self.classifier(output)  # Classify the aggregated outputs\n",
    "        return output  # Return the final model output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Ablation Study\n",
    "An ablation study is conducted using the Chet_TF model to evaluate the importance of the transition layer in the original Chet model. The key modification in Chet_TF is the replacement of the complex transition layer (typically involving GRU or similar mechanisms) with a simpler linear transition layer. This change aims to dissect the contribution of sophisticated transition dynamics to the overall performance of the model and to understand whether similar results can be achieved with less complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chet_TF(nn.Module):\n",
    "    def __init__(self, code_num, code_size, adj, graph_size, hidden_size,\n",
    "                output_size, dropout_rate, activation, t_attention_size, t_output_size):\n",
    "        super().__init__()\n",
    "        # Initialize embedding, graph, and the linear transition layers\n",
    "        self.embedding_layer = EmbeddingLayer(code_num, code_size, graph_size)\n",
    "        self.graph_layer = GraphLayer(adj, code_size, graph_size)\n",
    "        self.linear_transition = nn.Linear(graph_size, hidden_size)  # Linear layer to replace transition function\n",
    "        self.tanh = nn.Tanh()  # Tanh activation function\n",
    "        self.attention = DotProductAttention(hidden_size, 32)  # Dot product attention mechanism\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate, activation)  # Classifier component\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        # Generate embeddings from the embedding layer\n",
    "        embeddings = self.embedding_layer()\n",
    "        c_embeddings, n_embeddings, u_embeddings = embeddings\n",
    "        output = []\n",
    "        # Process each sequence in the batch\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            output_i = []\n",
    "            # Iterate over time steps within a sequence\n",
    "            for c_it, n_it in zip(code_x_i, neighbor_i):\n",
    "                # Process current time step using the graph layer\n",
    "                co_embeddings, _ = self.graph_layer(c_it, n_it, c_embeddings, n_embeddings)\n",
    "                # Apply the linear layer and then tanh activation\n",
    "                transformed_output = self.tanh(self.linear_transition(co_embeddings))\n",
    "                output_i.append(transformed_output)  # Collect outputs for each time step\n",
    "            # Apply attention to the sequence of outputs\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)  # Collect final outputs for all sequences\n",
    "        output = torch.vstack(output)  # Stack all sequence outputs\n",
    "        output = self.classifier(output)  # Classify the aggregated outputs\n",
    "        return output  # Return the final model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "`hidden_size`: The size of the hidden layers within neural network components like GRUs, set at 100. This parameter is crucial for capturing complex patterns and dependencies in the data. <br>\n",
    "`t_attention_size`: The size of the attention layers within the model, set at 32. It influences how the model focuses on different parts of the input data.<br>\n",
    "`batch_size`: Set at 32, affecting how many examples are processed at once during training. This can impact both training speed and model stability.<br>\n",
    "`epochs`: The total number of cycles the training process runs through the entire dataset, set at 100. More epochs can lead to better learning but also a risk of overfitting.<br>\n",
    "`dropout_rate`: Set at 0.0, indicating no dropout is used in this configuration. Dropout is a technique used to prevent overfitting by randomly setting a fraction of the input units to zero during training.<br>\n",
    "`dynamic learning_rate` <br>\n",
    "  - Initial Learning Rate (init_lr): Set at 0.01. This is the starting learning rate when training begins. An initial rate of 0.01 is fairly standard, providing a balance between training speed and the risk of overshooting the minimum of the loss function.\n",
    "  - Learning Rate Milestones: These are specific epochs where the learning rate is scheduled to change. In your setup, the milestones are at epochs 2, 3, 20, and 100.\n",
    "  - Learning Rate Steps: At each milestone, the learning rate changes to a new value:\n",
    "    - After epoch 2, the learning rate changes to 1e-3.\n",
    "    - After epoch 3, it changes again to 1e-4.\n",
    "    - After epoch 20, it adjusts to 1e-5. \n",
    "\n",
    "#### Computation Requirements\n",
    "\n",
    "VM Configurations on Google Cloud Platform:\n",
    "| Component       | Specification       |\n",
    "|-----------------|---------------------|\n",
    "| CPU             | 16 Core             |\n",
    "| CPU Platform    | Intel Broadwell     |\n",
    "| Memory          | 104 GB              |\n",
    "| GPU             | 1 x NVIDIA Tesla P100 |\n",
    "\n",
    "\n",
    "#### Training Time:\n",
    "\n",
    "| Metric               | Chet       | Chet_TF    |\n",
    "|----------------------|------------|------------|\n",
    "| Epoch Run time       | 55 seconds | 13 mins    |\n",
    "| Total Run time       | 30 mins    | 9 hours    |\n",
    "| Number of Epochs run* | 32         | 42         |\n",
    "\n",
    "* The number of epoches run varies due to early stop intervention when the model doesn't improve after 30 epoches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation Code\n",
    "\n",
    "#### Metric Descriptions\n",
    "\n",
    "1. Average Loss\n",
    "- Description: The average loss is calculated using a specified loss function (loss_fn), which measures the discrepancy between the predicted outputs (output) and the actual target values (y). This loss is aggregated across all batches to provide a holistic measure of the modelâ€™s prediction accuracy in terms of error magnitude.\n",
    "- Role: It provides a direct measure of the model's prediction error, with lower values indicating better performance. This metric is crucial for understanding how well the model fits the data.\n",
    "2. AUC (Area Under the ROC Curve)\n",
    "- Description: The AUC metric measures the ability of the model to discriminate between the classes (e.g., heart failure vs. no heart failure). It is calculated by plotting the true positive rate against the false positive rate at various threshold settings.\n",
    "- Role: AUC is particularly useful in binary classification problems with imbalanced classes. A higher AUC value (close to 1.0) indicates that the model has a good measure of separability between positive and negative classes. It shows how well the model can distinguish between two diagnostic categories, which is vital for clinical decision-making tasks.\n",
    "3. F1 Score\n",
    "- Description: The F1 score is the harmonic mean of precision and recall, providing a balance between these two metrics. It is particularly useful when the class distribution is uneven.\n",
    "- Role: The F1 score gives an idea of how precisely and robustly the model identifies the positive class while managing the trade-off between precision (the accuracy of positive predictions) and recall (the ability to find all actual positives). A higher F1 score indicates better precision and recall balance, which is crucial for medical diagnostics where both missing a condition (low recall) and falsely diagnosing a condition (low precision) have significant implications.\n",
    "4. Composite Score\n",
    "- A customized evaluation metric of weighted sum of loss_fn, AUC and F1 Score. It's a reasonable because of the following:\n",
    "  - The data is imbalance of 1:3 ratio of Heart Failure vs total samples. Both AUC and F1 score are great metrics in this imbalance dataset situation.\n",
    "  - The heart failure prediction is a binary classification problem. AUC can help understand the model's capability to rank predictions correctly.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(loss, auc, f1):\n",
    "    \"\"\" Normalize metrics to be in the same scale, higher is better \"\"\"\n",
    "    norm_loss = 1 / (1 + loss)  # Transform loss so that higher is better\n",
    "    return norm_loss, auc, f1\n",
    "\n",
    "def compute_composite_score(loss, auc, f1, weights={'loss': 0.2, 'auc': 0.4, 'f1': 0.4}):\n",
    "    \"\"\" Compute a weighted sum of normalized scores \"\"\"\n",
    "    norm_loss, norm_auc, norm_f1 = normalize_scores(loss, auc, f1)\n",
    "    composite_score = (weights['loss'] * norm_loss +\n",
    "                       weights['auc'] * norm_auc +\n",
    "                       weights['f1'] * norm_f1)\n",
    "    return composite_score\n",
    "\n",
    "def evaluate_hf(model, dataset, loss_fn, output_size=1, historical=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    labels = dataset.label()\n",
    "    outputs = []\n",
    "    preds = []\n",
    "    for step in range(len(dataset)):\n",
    "        code_x, visit_lens, divided, y, neighbors = dataset[step]\n",
    "        output = model(code_x, divided, neighbors, visit_lens).squeeze()\n",
    "        if output.dim() == 0:  # output is a scalar\n",
    "            output = output.unsqueeze(0)\n",
    "        loss = loss_fn(output, y)\n",
    "        total_loss += loss.item() * output_size * len(code_x)\n",
    "        outputs.append(output.detach().cpu().numpy())\n",
    "        pred = (output > 0.5).int()\n",
    "        preds.append(pred.cpu().numpy())  # Ensure tensor is moved to CPU before conversion\n",
    "        print('\\r    Evaluating step %d / %d' % (step + 1, len(dataset)), end='')\n",
    "    #print(\"dataset size: \", dataset.size())\n",
    "    avg_loss = total_loss / dataset.size()\n",
    "    outputs = np.concatenate(outputs)\n",
    "    preds = np.concatenate(preds)\n",
    "    auc = roc_auc_score(labels, outputs)\n",
    "    f1_score_ = f1_score(labels, preds)\n",
    "    print('\\r    Evaluation: loss: %.4f --- auc: %.4f --- f1_score: %.4f' % (avg_loss, auc, f1_score_))\n",
    "    current_score = compute_composite_score(avg_loss, auc, f1_score_)\n",
    "    return current_score, avg_loss, auc, f1_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_adj, EHRDataset, format_time, MultiStepLRScheduler\n",
    "\n",
    "def historical_hot(code_x, code_num, lens):\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, (x, l) in enumerate(zip(code_x, lens)):\n",
    "        result[i] = x[l - 1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size):\n",
    "    seed = 1000\n",
    "    dataset = 'mimic3'  # 'mimic3' or 'eicu'\n",
    "    task = 'h'  # 'h' for heart failure prediction\n",
    "    model_name = model_name\n",
    "    \n",
    "\n",
    "\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "    print('device:', device)\n",
    "\n",
    "    code_size = code_size\n",
    "    graph_size = graph_size\n",
    "    hidden_size = hidden_size  # rnn hidden size\n",
    "    t_attention_size = t_attention_size\n",
    "    t_output_size = hidden_size\n",
    "    batch_size = batch_size\n",
    "    epochs = epochs\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    dataset_path = os.path.join('data', dataset, 'standard')\n",
    "    train_path = os.path.join(dataset_path, 'train')\n",
    "    valid_path = os.path.join(dataset_path, 'valid')\n",
    "    test_path = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    code_adj = load_adj(dataset_path, device=device)\n",
    "    code_num = len(code_adj)\n",
    "    print('loading train data ...')\n",
    "    train_data = EHRDataset(train_path, label=task, batch_size=batch_size, shuffle=True, device=device)\n",
    "    print('loading valid data ...')\n",
    "    valid_data = EHRDataset(valid_path, label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "    print('loading test data ...')\n",
    "    test_data = EHRDataset(test_path, label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "\n",
    "    test_historical = historical_hot(valid_data.code_x, code_num, valid_data.visit_lens)\n",
    "\n",
    "    task_conf = {\n",
    "        'h': {\n",
    "            'dropout': 0.0,\n",
    "            'output_size': 1,\n",
    "            'evaluate_fn': evaluate_hf,\n",
    "            'lr': {\n",
    "                'init_lr': 0.01,\n",
    "                'milestones': [2, 3, 20],\n",
    "                'lrs': [1e-3, 1e-4, 1e-5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    model_select = {\n",
    "        'Chet': Chet,\n",
    "        'Chet_TF': Chet_TF,\n",
    "    }\n",
    "\n",
    "    output_size = task_conf[task]['output_size']\n",
    "    activation = torch.nn.Sigmoid()\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    evaluate_fn = task_conf[task]['evaluate_fn']\n",
    "    dropout_rate = task_conf[task]['dropout']\n",
    "\n",
    "    param_path = os.path.join('data', 'params', dataset, task, model_name)\n",
    "    if not os.path.exists(param_path):\n",
    "        os.makedirs(param_path)\n",
    "\n",
    "    model = model_select[model_name](code_num=code_num, code_size=code_size,\n",
    "                    adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                    t_output_size=t_output_size,\n",
    "                    output_size=output_size, dropout_rate=dropout_rate, activation=activation).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    print(\"Epochs:\", epochs)\n",
    "    print(\"Initial LR:\", task_conf[task]['lr']['init_lr'])\n",
    "    print(\"Milestones:\", task_conf[task]['lr']['milestones'])\n",
    "    print(\"Learning Rates:\", task_conf[task]['lr']['lrs'])\n",
    "\n",
    "    scheduler = MultiStepLRScheduler(optimizer, epochs, task_conf[task]['lr']['init_lr'],\n",
    "                                        task_conf[task]['lr']['milestones'], task_conf[task]['lr']['lrs'])\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "\n",
    "    best_score = float('-inf')  # Initialize to a large number\n",
    "    best_epoch = -1  # Track the epoch at which the best model was found\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %d / %d:' % (epoch + 1, epochs))\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_num = 0\n",
    "        steps = len(train_data)\n",
    "        st = time.time()\n",
    "        scheduler.step()\n",
    "        for step in range(len(train_data)):\n",
    "            optimizer.zero_grad()\n",
    "            code_x, visit_lens, divided, y, neighbors = train_data[step]\n",
    "            output = model(code_x, divided, neighbors, visit_lens).squeeze()\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * output_size * len(code_x)\n",
    "            total_num += len(code_x)\n",
    "\n",
    "            end_time = time.time()\n",
    "            remaining_time = format_time((end_time - st) / (step + 1) * (steps - step - 1))\n",
    "            print('\\r    Step %d / %d, remaining time: %s, loss: %.4f'\n",
    "                    % (step + 1, steps, remaining_time, total_loss / total_num), end='')\n",
    "        train_data.on_epoch_end()\n",
    "        et = time.time()\n",
    "        time_cost = format_time(et - st)\n",
    "        print('\\r    Step %d / %d, time cost: %s, loss: %.4f' % (steps, steps, time_cost, total_loss / total_num))\n",
    "\n",
    "        # Evaluate the model on validation data\n",
    "        # Set model to evaluation mode for validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "            current_score = evaluate_fn(model, valid_data, loss_fn, output_size, test_historical)\n",
    "            print('\\r    composite score is: %.4f' % current_score)\n",
    "\n",
    "        # Save the model only if the validation loss improved\n",
    "        # Check if current score is better; if so, save the model\n",
    "        if current_score > best_score:\n",
    "            print(f'Improved from {best_score:.4f} to {current_score:.4f}. Saving model...')\n",
    "            best_score = current_score\n",
    "            best_epoch = epoch\n",
    "            model_save_path = os.path.join(param_path, 'best_model.pt')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            no_improvement_count = 0  # Reset counter\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f'No improvement. Count: {no_improvement_count}/{20}')\n",
    "\n",
    "        if no_improvement_count >= 20:\n",
    "            print(f'Stopping early after {epoch+1} epochs.')\n",
    "            break  # Exit the loop if no improvement in the last 20 epochs\n",
    "            \n",
    "\n",
    "        \n",
    "    # print out the best epoch and its performance after training is complete\n",
    "    print(f'Best performing model was at epoch {best_epoch + 1} with a score of {best_score:.4f}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chet Model (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "Loading model...\n",
      "Model loaded from data/params/mimic3/h/Chet/best_model.pt and moved to mps\n",
      "Chet(\n",
      "  (embedding_layer): EmbeddingLayer()\n",
      "  (graph_layer): GraphLayer(\n",
      "    (dense): Linear(in_features=48, out_features=32, bias=True)\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (transition_layer): TransitionLayer(\n",
      "    (gru): GRUCell(32, 100)\n",
      "    (single_head_attention): SingleHeadAttentionLayer(\n",
      "      (dense_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (dense_k): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (dense_v): Linear(in_features=32, out_features=100, bias=True)\n",
      "    )\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (attention): DotProductAttention(\n",
      "    (dense): Linear(in_features=100, out_features=32, bias=True)\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (activation): Sigmoid()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters: 675185\n",
      "Layer: embedding_layer.c_embeddings | Size: torch.Size([4880, 48]) | Total parameters: 234240\n",
      "Layer: embedding_layer.n_embeddings | Size: torch.Size([4880, 48]) | Total parameters: 234240\n",
      "Layer: embedding_layer.u_embeddings | Size: torch.Size([4880, 32]) | Total parameters: 156160\n",
      "Layer: graph_layer.dense.weight | Size: torch.Size([32, 48]) | Total parameters: 1536\n",
      "Layer: graph_layer.dense.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: transition_layer.gru.weight_ih | Size: torch.Size([300, 32]) | Total parameters: 9600\n",
      "Layer: transition_layer.gru.weight_hh | Size: torch.Size([300, 100]) | Total parameters: 30000\n",
      "Layer: transition_layer.gru.bias_ih | Size: torch.Size([300]) | Total parameters: 300\n",
      "Layer: transition_layer.gru.bias_hh | Size: torch.Size([300]) | Total parameters: 300\n",
      "Layer: transition_layer.single_head_attention.dense_q.weight | Size: torch.Size([32, 32]) | Total parameters: 1024\n",
      "Layer: transition_layer.single_head_attention.dense_q.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: transition_layer.single_head_attention.dense_k.weight | Size: torch.Size([32, 32]) | Total parameters: 1024\n",
      "Layer: transition_layer.single_head_attention.dense_k.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: transition_layer.single_head_attention.dense_v.weight | Size: torch.Size([100, 32]) | Total parameters: 3200\n",
      "Layer: transition_layer.single_head_attention.dense_v.bias | Size: torch.Size([100]) | Total parameters: 100\n",
      "Layer: attention.context | Size: torch.Size([32, 1]) | Total parameters: 32\n",
      "Layer: attention.dense.weight | Size: torch.Size([32, 100]) | Total parameters: 3200\n",
      "Layer: attention.dense.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: classifier.linear.weight | Size: torch.Size([1, 100]) | Total parameters: 100\n",
      "Layer: classifier.linear.bias | Size: torch.Size([1]) | Total parameters: 1\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Chet'\n",
    "model_path = os.path.join('data', 'params', 'mimic3', 'h', model_name, 'best_model.pt')\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "code_size = 48\n",
    "graph_size = 32\n",
    "hidden_size = 100  # rnn hidden size\n",
    "t_attention_size = 32\n",
    "t_output_size = hidden_size\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "output_size = 1\n",
    "activation = torch.nn.Sigmoid()\n",
    "dropout_rate = 0.0\n",
    "\n",
    "#model_Chet = train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size)\n",
    "model_Chet = Chet(code_num=code_num, code_size=code_size,\n",
    "                  adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                  t_output_size=t_output_size,\n",
    "                  output_size=output_size, dropout_rate=dropout_rate, activation=activation).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Load the model if it exists\n",
    "    print(\"Loading model...\")\n",
    "    model_Chet.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_Chet.to(device)\n",
    "    print(f'Model loaded from {model_path} and moved to {device}')\n",
    "\n",
    "else:\n",
    "    # Train the model if the file does not exist\n",
    "    # Assuming train_data, optimizer, and loss_fn are defined elsewhere\n",
    "    print('Model is not available, starting training...')\n",
    "    model_Chet = train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size)\n",
    "    \n",
    "    # After training, save the model to the specified path\n",
    "    print(f'Saving model to {model_path}')\n",
    "    torch.save(model_Chet.state_dict(), model_path)\n",
    "    \n",
    "\n",
    "print(model_Chet)\n",
    "total_params = sum(p.numel() for p in model_Chet.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "for name, param in model_Chet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()} | Total parameters: {param.numel()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chet_TF - Albation Model (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "Model loaded from data/params/mimic3/h/Chet_TF/best_model.pt and moved to mps\n",
      "Chet_TF(\n",
      "  (embedding_layer): EmbeddingLayer()\n",
      "  (graph_layer): GraphLayer(\n",
      "    (dense): Linear(in_features=48, out_features=32, bias=True)\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (linear_transition): Linear(in_features=32, out_features=100, bias=True)\n",
      "  (tanh): Tanh()\n",
      "  (attention): DotProductAttention(\n",
      "    (dense): Linear(in_features=100, out_features=32, bias=True)\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (activation): Sigmoid()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters: 632873\n",
      "Layer: embedding_layer.c_embeddings | Size: torch.Size([4880, 48]) | Total parameters: 234240\n",
      "Layer: embedding_layer.n_embeddings | Size: torch.Size([4880, 48]) | Total parameters: 234240\n",
      "Layer: embedding_layer.u_embeddings | Size: torch.Size([4880, 32]) | Total parameters: 156160\n",
      "Layer: graph_layer.dense.weight | Size: torch.Size([32, 48]) | Total parameters: 1536\n",
      "Layer: graph_layer.dense.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: linear_transition.weight | Size: torch.Size([100, 32]) | Total parameters: 3200\n",
      "Layer: linear_transition.bias | Size: torch.Size([100]) | Total parameters: 100\n",
      "Layer: attention.context | Size: torch.Size([32, 1]) | Total parameters: 32\n",
      "Layer: attention.dense.weight | Size: torch.Size([32, 100]) | Total parameters: 3200\n",
      "Layer: attention.dense.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: classifier.linear.weight | Size: torch.Size([1, 100]) | Total parameters: 100\n",
      "Layer: classifier.linear.bias | Size: torch.Size([1]) | Total parameters: 1\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Chet_TF'\n",
    "model_path = os.path.join('data', 'params', 'mimic3', 'h', model_name, 'best_model.pt')\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "code_size = 48\n",
    "graph_size = 32\n",
    "hidden_size = 100  # rnn hidden size\n",
    "t_attention_size = 32\n",
    "t_output_size = hidden_size\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "output_size = 1\n",
    "activation = torch.nn.Sigmoid()\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model_tf = Chet_TF(code_num=code_num, code_size=code_size,\n",
    "                    adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                    t_output_size=t_output_size,\n",
    "                    output_size=output_size, dropout_rate=dropout_rate, activation=activation).to(device)\n",
    "\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Load the model if it exists\n",
    "    model_tf.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_tf.to(device)\n",
    "    print(f'Model loaded from {model_path} and moved to {device}')\n",
    "else:\n",
    "    # Train the model if the file does not exist\n",
    "    # Assuming train_data, optimizer, and loss_fn are defined elsewhere\n",
    "    print('Model is not available, starting training...')\n",
    "    model_tf = train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size)\n",
    "\n",
    "    # After training, save the model to the specified path\n",
    "    print(f'Saving model to {model_path}')\n",
    "    torch.save(model_tf.state_dict(), model_path)\n",
    "\n",
    "print(model_tf)\n",
    "total_params = sum(p.numel() for p in model_tf.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "for name, param in model_tf.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()} | Total parameters: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "## Results and Discussion\n",
    "\n",
    "Evaluate the two models Chet and Chet_TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: loss: 0.8693 --- auc: 0.7958 --- f1_score: 0.6024\n",
      "    Evaluating step 1 / 32"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     _, test_loss_chet, auc_chet, f1_score_chet \u001b[38;5;241m=\u001b[39m evaluate_hf(model_Chet, test_data, loss_fn, output_size, test_historical)\n\u001b[0;32m---> 12\u001b[0m     _, test_loss_tf, auc_tf, f1_score_tf \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_historical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print the evaluation results\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChet model test loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_chet\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_chet\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1 score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score_chet\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)    \n",
      "File \u001b[0;32m~/Documents/MCS/DLH598/project_chet/CS598-DLH-Chet/metrics.py:103\u001b[0m, in \u001b[0;36mevaluate_hf\u001b[0;34m(model, dataset, loss_fn, output_size, historical)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[1;32m    102\u001b[0m     code_x, visit_lens, divided, y, neighbors \u001b[38;5;241m=\u001b[39m dataset[step]\n\u001b[0;32m--> 103\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisit_lens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# output is a scalar\u001b[39;00m\n\u001b[1;32m    105\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mChet_TF.forward\u001b[0;34m(self, code_x, divided, neighbors, lens)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Iterate over time steps within a sequence\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c_it, n_it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(code_x_i, neighbor_i):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Process current time step using the graph layer\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     co_embeddings, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_it\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_it\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Apply the linear layer and then tanh activation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     transformed_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_transition(co_embeddings))\n",
      "File \u001b[0;32m~/anaconda3/envs/python3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mGraphLayer.forward\u001b[0;34m(self, code_x, neighbor, c_embeddings, n_embeddings)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Multiply embeddings by adjacency matrix to propagate through the graph\u001b[39;00m\n\u001b[1;32m     24\u001b[0m cc_embeddings \u001b[38;5;241m=\u001b[39m center_codes \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj, center_embeddings)\n\u001b[0;32m---> 25\u001b[0m cn_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcenter_codes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbor_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m nn_embeddings \u001b[38;5;241m=\u001b[39m neighbor_codes \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj, neighbor_embeddings)\n\u001b[1;32m     27\u001b[0m nc_embeddings \u001b[38;5;241m=\u001b[39m neighbor_codes \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj, center_embeddings)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join('data', dataset, 'standard')\n",
    "test_path = os.path.join(dataset_path, 'test')\n",
    "test_data = EHRDataset(test_path, label='h', batch_size=32, shuffle=False, device=device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "output_size = 1\n",
    "code_adj = load_adj(dataset_path, device=device)\n",
    "code_num = len(code_adj)\n",
    "test_historical = historical_hot(test_data.code_x, code_num, test_data.visit_lens)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, test_loss_chet, auc_chet, f1_score_chet = evaluate_hf(model_Chet, test_data, loss_fn, output_size, test_historical)\n",
    "    _, test_loss_tf, auc_tf, f1_score_tf = evaluate_hf(model_tf, test_data, loss_fn, output_size, test_historical)\n",
    "    \n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Chet model test loss: {test_loss_chet:.4f}, AUC : {auc_chet:4f}, F1 score: {f1_score_chet:.4f}')    \n",
    "print(f'Chet_TF model test loss: {test_loss_tf:.4f}, AUC : {auc_tf:4f}, F1 score: {f1_score_tf:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric    | Chet     | Chet_TF  |\n",
    "|-----------|----------|----------|\n",
    "| Test Loss | 0.869307 | 0.597295 |\n",
    "| AUC       | 0.795823 | 0.763171 |\n",
    "| F1 Score  | 0.602410 | 0.719577 |\n",
    "| # Params  | 0.68M    | 0.63M    |\n",
    "            Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison to paper results\n",
    "\n",
    "<p align=\"left\"> <img src=\"images\\image_2.png\" alt=\"Image 2\" width=\"300\" height=\"200\"> </p>\n",
    "Figure 1<br><br>\n",
    "\n",
    "The results of this project (Table 1) convincingly demonstrate that the Chet model, as implemented, not only replicates the performance benchmarks set by the original research but also surpasses the capabilities of traditional models (Figure 1) in predicting health events such as heart failure. This enhancement is particularly significant given the complexity of the medical datasets involved and the critical need for accurate predictive models in healthcare. The Chet modelâ€™s advanced features, including its use of dynamic disease graphs and attention mechanisms, provide a more nuanced understanding of patient data compared to traditional models, which often rely on static or less sophisticated methods of analysis.\n",
    "\n",
    "The superior performance of the Chet model, evidenced by metrics such as AUC and F1 score, suggests that integrating complex data representations and leveraging relationships between medical conditions can yield substantial improvements in predictive accuracy. This finding is pivotal for clinical applications, where the early and precise prediction of health events can lead to better patient outcomes and more effective resource allocation. Additionally, the project's success in implementing such an advanced model underscores the potential for deep learning techniques to transform medical diagnostics and prognostics, setting a new standard for subsequent models and applications in the field.\n",
    "\n",
    "Moreover, the ability of the Chet model to outperform traditional approaches validates the ongoing shift towards more data-driven, personalized healthcare solutions. It encourages further research and development in this direction, potentially leading to broader adoption and refinement of similar models in other areas of healthcare. This success story adds to the growing body of evidence that complex models, while demanding in terms of computational resources and data requirements, offer indispensable benefits that are unattainable with simpler methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Albation Study Discussion\n",
    "\n",
    "<p align=\"left\"> <img src=\"images\\image_3.png\" alt=\"Image 2\" width=\"500\" height=\"200\"> </p>\n",
    "Figure 2<br><br>\n",
    "\n",
    "The results of the ablation study involving the Chet_TF model (Figure 1), which substitutes the transition layer with a linear layer rather than a direct sum Graph Neural Network (GNN) as described in the original paper as Chet<sub>t-</sub> (Figure 2) [1], offer insightful revelations about the architectural nuances of the Chet model. Despite this deviation from the paper's methodology, the Chet_TF model's performance, though slightly inferior to that of the complete Chet model, underscores the critical role of the transition layer in enhancing the model's predictive accuracy.\n",
    "\n",
    "This slight underperformance of the Chet_TF model highlights the substantial impact that sophisticated transition mechanisms can have on capturing the dynamic changes in patient health states over time. The use of a simple linear layer, while computationally less complex, evidently lacks the depth required to model the intricate temporal interactions between different medical conditions as effectively as the original transition layer or a direct sum GNN. This finding reinforces the notion that while simplifications in model architecture can yield computationally efficient alternatives, they might also lead to diminished efficacy in scenarios where the evolution of patient states is highly non-linear and influenced by numerous interdependent factors.\n",
    "\n",
    "The results from this ablation study are crucial for understanding the trade-offs between model complexity and performance. They suggest that further explorations into different types of transition layers could potentially strike a better balance, offering models that are both computationally efficient and highly accurate. Such insights are invaluable for guiding future developments in predictive healthcare models, where the accuracy of predictions can significantly influence clinical decision-making and patient outcomes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "#### What was easy\n",
    "\n",
    "One of the straightforward aspects of this project was the accessibility of the original code repository for the Chet model. Having direct access to the implementation details facilitated the replication and understanding of the model, significantly reducing the complexity of translating theoretical descriptions into practical code.\n",
    "\n",
    "The codebase included not just the core modeling components but also preprocessing routines, evaluation metrics, and training procedures. This comprehensive framework allowed for focused experimentation and fine-tuning rather than initial development, accelerating the project's progress. The ability to interact directly with the original implementation enabled the team to experiment with modifications aimed at enhancing model performance.\n",
    "\n",
    "Through several trial runs, it became clear that adjustments in hyperparameters, such as learning rate schedules or dropout rates, could improve the modelâ€™s predictive accuracy and robustness. These test trials were instrumental in fine-tuning the model to better suit specific datasets and objectives, demonstrating that while the base model provided a solid foundation, targeted tweaks could further optimize its effectiveness. This iterative process of testing and modification was crucial for refining the Chet model into a more powerful tool for predictive healthcare analytics.\n",
    "\n",
    "\n",
    "#### What was hard\n",
    "\n",
    "The most challenging aspect of this project was grasping the complex mathematical concepts underlying the implementations of the Chet model. While the original code repository provided a solid foundation for practical implementation, delving into the theoretical mathematics that drove these implementations required a considerable amount of effort. The model's reliance on advanced techniques in graph theory, deep learning, and statistical methods meant that a deep understanding of these areas was essential to fully comprehend how the model functions and why it was designed in a particular way.\n",
    "\n",
    "To tackle this challenge, it was extremely beneficial to deconstruct the model into its individual components and study them in isolation. By methodically analyzing each part of the modelâ€”such as the embedding layers, graph-based processing layers, and transition mechanismsâ€”we could directly relate practical implementations back to the theoretical concepts discussed in the original paper. This approach allowed for a more manageable learning curve, as each component could be understood within the context of its role in the overall model architecture.\n",
    "\n",
    "Learning these components separately and then integrating them provided a comprehensive understanding of how the model's design achieved its objectives. This step-by-step unpacking of the model's architecture was critical in translating complex mathematical formulations into tangible, implementable code, ensuring that the enhancements and optimizations made were both theoretically sound and practically viable.\n",
    "\n",
    "\n",
    "#### Recommendation for improving reproducibility\n",
    "\n",
    "To enhance the reproducibility of the Chet model, detailed documentation and thorough annotations or comments within the codebase are essential. These annotations should clarify the purpose and function of each part of the code, helping others to follow the logic and modifications easily. Sharing the annotated code on a publicly accessible platform like GitHub, complete with a requirements.txt file for environment setup, would significantly aid in transparency. Implementing version control to document changes and rationale behind each update is also crucial. Additionally, providing a detailed setup and execution guide, along with access to example or synthetic datasets that replicate the original data's characteristics, would further facilitate accurate replication of the study's results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public GitHub Repo\n",
    "\n",
    "This project's code is accessable on [Github] (https://github.com/ericyan3000/CS598-DLH-Chet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] Lu, Chang, Tian Han, and Yue Ning. \"Context-aware health event prediction via transition functions on dynamic disease graphs.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 4, pp. 4567-4574. 2022. <br>\n",
    "[2] Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe and Svetha Venkatesh, \" Deepr : A Convolutional Net for Medical Records,\" in IEEE Journal of Biomedical and Health Informatics, vol. 21, no. 1, pp. 22-30, Jan. 2017.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5",
     "timestamp": 1709153069464
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
