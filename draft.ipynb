{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "<h1 align=\"center\">CS598: Deep Learning for Healthcare Final Project</h1>\n",
    "<h3 align=\"center\">Team #164</h3>\n",
    "<h3 align=\"center\">Team member: Zexi Yan, Stanley Wen, and Bo Zhang</h3>\n",
    "<h3 align=\"center\">Corresponding emails: zexiyan2@illinois.edu, keyanw2@illinois.edu, zhang375@illinois.edu</h3>\n",
    "<h3 align=\"center\">Github Repo: https://github.com/ericyan3000/CS598-DLH-Chet.git</h3>\n",
    "\n",
    "\n",
    "\n",
    "# Introduction\n",
    "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
    "\n",
    "###   Background of the problem\n",
    "The primary challenge addressed by the paper is the limitation of existing health event prediction models which consider diagnoses as independent entities, neglecting the clinical relationships among diseases. This oversight hinders the ability to effectively utilize combinational disease information and understand the dynamic nature of disease development over time. This leads to the two problems the paper is trying to address.\n",
    "- Disease combinations in medical practice form a global graph structure that reveals hidden patterns among diseases, with individual patient visits represented as local subgraphs. Despite the potential to predict future health events by analyzing these structures, common deep learning models like GRAM [2], Timeline [3], and G-BERT [4] do not utilize this graph structure for health event predictions.\n",
    "- The progression of a disease in a patient is dynamic, as evidenced by changing diagnosis priorities and the emergence of new diagnoses in EHR datasets like MIMIC-III [5]. This dynamic nature, where diseases evolve and impact patients differently over time, suggests the need for a model that can dynamically represent disease development and learn the transition from potential to actual diagnoses.\n",
    "###   Paper explanation\n",
    "The paper titled \"Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs\" [1] introduces a novel framework for improving health event predictions by incorporating dynamic disease relationships within EHR data. The authors propose a sophisticated model that constructs and utilizes dynamic disease graphs to represent the evolving relationships between different diagnoses as patients continue to visit healthcare facilities.\n",
    "\n",
    "The innovation of the method lies in its ability to dynamically adjust disease representations and interactions based on a patient's history and current health state. This is achieved through the use of global disease co-occurrence graphs and patient-specific subgraphs, which adapt based on new health information. The model uses transition functions to model the changes in disease relevance and connections, reflecting the natural progression and regression of disease states over time. \n",
    "\n",
    "In terms of effectiveness, the proposed method has shown to outperform existing models significantly, as demonstrated through rigorous testing on real-world EHR datasets. The results indicated improvements in prediction accuracy for various health events, showcasing the model's capability to handle the complex dynamics of disease progression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "1. Chet can outperform existing state-of-the-art health event prediction models by effectively utilizing the dynamic and combinational nature of disease information. Specifically, we will compare Chet with the following model:\n",
    "     - CNN-based model: Deepr [2].\n",
    "\n",
    "2. The introduction of transition functions and dynamic graph learning will provide a more nuanced understanding of disease progression, leading to more accurate health event predictions. This will be tested by the Ablations described below.\n",
    "     - Evaluate the model's performance without the transition functions on the prediction accuracy to understand their contribution to the modelâ€™s performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "### Data\n",
    "The same preprocessed data will be used to feed each model and for the same task of generating predictions heart failure events.\n",
    "\n",
    "### Deepr Model (Control)\n",
    "The Deepr model applies convolutional neural networks to sequences of diagnoses, treating each visit as a series of inputs to capture temporal and contextual patterns within patient data. This model serves as a baseline for comparison with more complex architectures.\n",
    "\n",
    "### Full Chet Model\n",
    "The Chet model integrates dynamic disease graphs with a graph neural network (GNN) to capture both the global disease relationships across all patients and local changes specific to individual visits. It employs transition functions to model the temporal evolution of diseases, providing a comprehensive framework for predicting future health events.\n",
    "\n",
    "### Chet Model without Transition Functions (Chet-TF)\n",
    "This variant of the Chet model retains the dynamic graph learning component but omits the transition functions. It focuses on the spatial relationships among diseases using GNN outputs directly for prediction, thus simplifying the approach by not modeling the temporal progression of diseases explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data\n",
    "  * Source of the data: MIMIC III data to be used. Since it's not available yet, a demo data of MIMIC III is used instead.\n",
    "    * https://physionet.org/content/mimiciii-demo/1.4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient num: 7493\n",
      "max admission num: 42\n",
      "mean admission num: 2.66\n",
      "max code num in an admission: 39\n",
      "mean code num in an admission: 13.06\n",
      "encoding code ...\n",
      "There are 4880 codes\n",
      "generating code levels ...\n",
      "train_num: 5500, test_num: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\t0.02%\r",
      "\t0.04%\r",
      "\t0.06%\r",
      "\t0.08%\r",
      "\t0.10%\r",
      "\t0.12%\r",
      "\t0.14%\r",
      "\t0.16%\r",
      "\t0.18%\r",
      "\t0.20%\r",
      "\t0.23%\r",
      "\t0.25%\r",
      "\t0.27%\r",
      "\t0.29%\r",
      "\t0.31%\r",
      "\t0.33%\r",
      "\t0.35%\r",
      "\t0.37%\r",
      "\t0.39%\r",
      "\t0.41%\r",
      "\t0.43%\r",
      "\t0.45%\r",
      "\t0.47%\r",
      "\t0.49%\r",
      "\t0.51%\r",
      "\t0.53%\r",
      "\t0.55%\r",
      "\t0.57%\r",
      "\t0.59%\r",
      "\t0.61%\r",
      "\t0.64%\r",
      "\t0.66%\r",
      "\t0.68%\r",
      "\t0.70%\r",
      "\t0.72%\r",
      "\t0.74%\r",
      "\t0.76%\r",
      "\t0.78%\r",
      "\t0.80%\r",
      "\t0.82%\r",
      "\t0.84%\r",
      "\t0.86%\r",
      "\t0.88%\r",
      "\t0.90%\r",
      "\t0.92%\r",
      "\t0.94%\r",
      "\t0.96%\r",
      "\t0.98%\r",
      "\t1.00%\r",
      "\t1.02%\r",
      "\t1.05%\r",
      "\t1.07%\r",
      "\t1.09%\r",
      "\t1.11%\r",
      "\t1.13%\r",
      "\t1.15%\r",
      "\t1.17%\r",
      "\t1.19%\r",
      "\t1.21%\r",
      "\t1.23%\r",
      "\t1.25%\r",
      "\t1.27%\r",
      "\t1.29%\r",
      "\t1.31%\r",
      "\t1.33%\r",
      "\t1.35%\r",
      "\t1.37%\r",
      "\t1.39%\r",
      "\t1.41%\r",
      "\t1.43%\r",
      "\t1.45%\r",
      "\t1.48%\r",
      "\t1.50%\r",
      "\t1.52%\r",
      "\t1.54%\r",
      "\t1.56%\r",
      "\t1.58%\r",
      "\t1.60%\r",
      "\t1.62%\r",
      "\t1.64%\r",
      "\t1.66%\r",
      "\t1.68%\r",
      "\t1.70%\r",
      "\t1.72%\r",
      "\t1.74%\r",
      "\t1.76%\r",
      "\t1.78%\r",
      "\t1.80%\r",
      "\t1.82%\r",
      "\t1.84%\r",
      "\t1.86%\r",
      "\t1.89%\r",
      "\t1.91%\r",
      "\t1.93%\r",
      "\t1.95%\r",
      "\t1.97%\r",
      "\t1.99%\r",
      "\t2.01%\r",
      "\t2.03%\r",
      "\t2.05%\r",
      "\t2.07%\r",
      "\t2.09%\r",
      "\t2.11%\r",
      "\t2.13%\r",
      "\t2.15%\r",
      "\t2.17%\r",
      "\t2.19%\r",
      "\t2.21%\r",
      "\t2.23%\r",
      "\t2.25%\r",
      "\t2.27%\r",
      "\t2.30%\r",
      "\t2.32%\r",
      "\t2.34%\r",
      "\t2.36%\r",
      "\t2.38%\r",
      "\t2.40%\r",
      "\t2.42%\r",
      "\t2.44%\r",
      "\t2.46%\r",
      "\t2.48%\r",
      "\t2.50%\r",
      "\t2.52%\r",
      "\t2.54%\r",
      "\t2.56%\r",
      "\t2.58%\r",
      "\t2.60%\r",
      "\t2.62%\r",
      "\t2.64%\r",
      "\t2.66%\r",
      "\t2.68%\r",
      "\t2.70%\r",
      "\t2.73%\r",
      "\t2.75%\r",
      "\t2.77%\r",
      "\t2.79%\r",
      "\t2.81%\r",
      "\t2.83%\r",
      "\t2.85%\r",
      "\t2.87%\r",
      "\t2.89%\r",
      "\t2.91%\r",
      "\t2.93%\r",
      "\t2.95%\r",
      "\t2.97%\r",
      "\t2.99%\r",
      "\t3.01%\r",
      "\t3.03%\r",
      "\t3.05%\r",
      "\t3.07%\r",
      "\t3.09%\r",
      "\t3.11%\r",
      "\t3.14%\r",
      "\t3.16%\r",
      "\t3.18%\r",
      "\t3.20%\r",
      "\t3.22%\r",
      "\t3.24%\r",
      "\t3.26%\r",
      "\t3.28%\r",
      "\t3.30%\r",
      "\t3.32%\r",
      "\t3.34%\r",
      "\t3.36%\r",
      "\t3.38%\r",
      "\t3.40%\r",
      "\t3.42%\r",
      "\t3.44%\r",
      "\t3.46%\r",
      "\t3.48%\r",
      "\t3.50%\r",
      "\t3.52%\r",
      "\t3.55%\r",
      "\t3.57%\r",
      "\t3.59%\r",
      "\t3.61%\r",
      "\t3.63%\r",
      "\t3.65%\r",
      "\t3.67%\r",
      "\t3.69%\r",
      "\t3.71%\r",
      "\t3.73%\r",
      "\t3.75%\r",
      "\t3.77%\r",
      "\t3.79%\r",
      "\t3.81%\r",
      "\t3.83%\r",
      "\t3.85%\r",
      "\t3.87%\r",
      "\t3.89%\r",
      "\t3.91%\r",
      "\t3.93%\r",
      "\t3.95%\r",
      "\t3.98%\r",
      "\t4.00%\r",
      "\t4.02%\r",
      "\t4.04%\r",
      "\t4.06%\r",
      "\t4.08%\r",
      "\t4.10%\r",
      "\t4.12%\r",
      "\t4.14%\r",
      "\t4.16%\r",
      "\t4.18%\r",
      "\t4.20%\r",
      "\t4.22%\r",
      "\t4.24%\r",
      "\t4.26%\r",
      "\t4.28%\r",
      "\t4.30%\r",
      "\t4.32%\r",
      "\t4.34%\r",
      "\t4.36%\r",
      "\t4.39%\r",
      "\t4.41%\r",
      "\t4.43%\r",
      "\t4.45%\r",
      "\t4.47%\r",
      "\t4.49%\r",
      "\t4.51%\r",
      "\t4.53%\r",
      "\t4.55%\r",
      "\t4.57%\r",
      "\t4.59%\r",
      "\t4.61%\r",
      "\t4.63%\r",
      "\t4.65%\r",
      "\t4.67%\r",
      "\t4.69%\r",
      "\t4.71%\r",
      "\t4.73%\r",
      "\t4.75%\r",
      "\t4.77%\r",
      "\t4.80%\r",
      "\t4.82%\r",
      "\t4.84%\r",
      "\t4.86%\r",
      "\t4.88%\r",
      "\t4.90%\r",
      "\t4.92%\r",
      "\t4.94%\r",
      "\t4.96%\r",
      "\t4.98%\r",
      "\t5.00%\r",
      "\t5.02%\r",
      "\t5.04%\r",
      "\t5.06%\r",
      "\t5.08%\r",
      "\t5.10%\r",
      "\t5.12%\r",
      "\t5.14%\r",
      "\t5.16%\r",
      "\t5.18%\r",
      "\t5.20%\r",
      "\t5.23%\r",
      "\t5.25%\r",
      "\t5.27%\r",
      "\t5.29%\r",
      "\t5.31%\r",
      "\t5.33%\r",
      "\t5.35%\r",
      "\t5.37%\r",
      "\t5.39%\r",
      "\t5.41%\r",
      "\t5.43%\r",
      "\t5.45%\r",
      "\t5.47%\r",
      "\t5.49%\r",
      "\t5.51%\r",
      "\t5.53%\r",
      "\t5.55%\r",
      "\t5.57%\r",
      "\t5.59%\r",
      "\t5.61%\r",
      "\t5.64%\r",
      "\t5.66%\r",
      "\t5.68%\r",
      "\t5.70%\r",
      "\t5.72%\r",
      "\t5.74%\r",
      "\t5.76%\r",
      "\t5.78%\r",
      "\t5.80%\r",
      "\t5.82%\r",
      "\t5.84%\r",
      "\t5.86%\r",
      "\t5.88%\r",
      "\t5.90%\r",
      "\t5.92%\r",
      "\t5.94%\r",
      "\t5.96%\r",
      "\t5.98%\r",
      "\t6.00%\r",
      "\t6.02%\r",
      "\t6.05%\r",
      "\t6.07%\r",
      "\t6.09%\r",
      "\t6.11%\r",
      "\t6.13%\r",
      "\t6.15%\r",
      "\t6.17%\r",
      "\t6.19%\r",
      "\t6.21%\r",
      "\t6.23%\r",
      "\t6.25%\r",
      "\t6.27%\r",
      "\t6.29%\r",
      "\t6.31%\r",
      "\t6.33%\r",
      "\t6.35%\r",
      "\t6.37%\r",
      "\t6.39%\r",
      "\t6.41%\r",
      "\t6.43%\r",
      "\t6.45%\r",
      "\t6.48%\r",
      "\t6.50%\r",
      "\t6.52%\r",
      "\t6.54%\r",
      "\t6.56%\r",
      "\t6.58%\r",
      "\t6.60%\r",
      "\t6.62%\r",
      "\t6.64%\r",
      "\t6.66%\r",
      "\t6.68%\r",
      "\t6.70%\r",
      "\t6.72%\r",
      "\t6.74%\r",
      "\t6.76%\r",
      "\t6.78%\r",
      "\t6.80%\r",
      "\t6.82%\r",
      "\t6.84%\r",
      "\t6.86%\r",
      "\t6.89%\r",
      "\t6.91%\r",
      "\t6.93%\r",
      "\t6.95%\r",
      "\t6.97%\r",
      "\t6.99%\r",
      "\t7.01%\r",
      "\t7.03%\r",
      "\t7.05%\r",
      "\t7.07%\r",
      "\t7.09%\r",
      "\t7.11%\r",
      "\t7.13%\r",
      "\t7.15%\r",
      "\t7.17%\r",
      "\t7.19%\r",
      "\t7.21%\r",
      "\t7.23%\r",
      "\t7.25%\r",
      "\t7.27%\r",
      "\t7.30%\r",
      "\t7.32%\r",
      "\t7.34%\r",
      "\t7.36%\r",
      "\t7.38%\r",
      "\t7.40%\r",
      "\t7.42%\r",
      "\t7.44%\r",
      "\t7.46%\r",
      "\t7.48%\r",
      "\t7.50%\r",
      "\t7.52%\r",
      "\t7.54%\r",
      "\t7.56%\r",
      "\t7.58%\r",
      "\t7.60%\r",
      "\t7.62%\r",
      "\t7.64%\r",
      "\t7.66%\r",
      "\t7.68%\r",
      "\t7.70%\r",
      "\t7.73%\r",
      "\t7.75%\r",
      "\t7.77%\r",
      "\t7.79%\r",
      "\t7.81%\r",
      "\t7.83%\r",
      "\t7.85%\r",
      "\t7.87%\r",
      "\t7.89%\r",
      "\t7.91%\r",
      "\t7.93%\r",
      "\t7.95%\r",
      "\t7.97%\r",
      "\t7.99%\r",
      "\t8.01%\r",
      "\t8.03%\r",
      "\t8.05%\r",
      "\t8.07%\r",
      "\t8.09%\r",
      "\t8.11%\r",
      "\t8.14%\r",
      "\t8.16%\r",
      "\t8.18%\r",
      "\t8.20%\r",
      "\t8.22%\r",
      "\t8.24%\r",
      "\t8.26%\r",
      "\t8.28%\r",
      "\t8.30%\r",
      "\t8.32%\r",
      "\t8.34%\r",
      "\t8.36%\r",
      "\t8.38%\r",
      "\t8.40%\r",
      "\t8.42%\r",
      "\t8.44%\r",
      "\t8.46%\r",
      "\t8.48%\r",
      "\t8.50%\r",
      "\t8.52%\r",
      "\t8.55%\r",
      "\t8.57%\r",
      "\t8.59%\r",
      "\t8.61%\r",
      "\t8.63%\r",
      "\t8.65%\r",
      "\t8.67%\r",
      "\t8.69%\r",
      "\t8.71%\r",
      "\t8.73%\r",
      "\t8.75%\r",
      "\t8.77%\r",
      "\t8.79%\r",
      "\t8.81%\r",
      "\t8.83%\r",
      "\t8.85%\r",
      "\t8.87%\r",
      "\t8.89%\r",
      "\t8.91%\r",
      "\t8.93%\r",
      "\t8.95%\r",
      "\t8.98%\r",
      "\t9.00%\r",
      "\t9.02%\r",
      "\t9.04%\r",
      "\t9.06%\r",
      "\t9.08%\r",
      "\t9.10%\r",
      "\t9.12%\r",
      "\t9.14%\r",
      "\t9.16%\r",
      "\t9.18%\r",
      "\t9.20%\r",
      "\t9.22%\r",
      "\t9.24%\r",
      "\t9.26%\r",
      "\t9.28%\r",
      "\t9.30%\r",
      "\t9.32%\r",
      "\t9.34%\r",
      "\t9.36%\r",
      "\t9.39%\r",
      "\t9.41%\r",
      "\t9.43%\r",
      "\t9.45%\r",
      "\t9.47%\r",
      "\t9.49%\r",
      "\t9.51%\r",
      "\t9.53%\r",
      "\t9.55%\r",
      "\t9.57%\r",
      "\t9.59%\r",
      "\t9.61%\r",
      "\t9.63%\r",
      "\t9.65%\r",
      "\t9.67%\r",
      "\t9.69%\r",
      "\t9.71%\r",
      "\t9.73%\r",
      "\t9.75%\r",
      "\t9.77%\r",
      "\t9.80%\r",
      "\t9.82%\r",
      "\t9.84%\r",
      "\t9.86%\r",
      "\t9.88%\r",
      "\t9.90%\r",
      "\t9.92%\r",
      "\t9.94%\r",
      "\t9.96%\r",
      "\t9.98%\r",
      "\t10.00%\r",
      "\t10.02%\r",
      "\t10.04%\r",
      "\t10.06%\r",
      "\t10.08%\r",
      "\t10.10%\r",
      "\t10.12%\r",
      "\t10.14%\r",
      "\t10.16%\r",
      "\t10.18%\r",
      "\t10.20%\r",
      "\t10.23%\r",
      "\t10.25%\r",
      "\t10.27%\r",
      "\t10.29%\r",
      "\t10.31%\r",
      "\t10.33%\r",
      "\t10.35%\r",
      "\t10.37%\r",
      "\t10.39%\r",
      "\t10.41%\r",
      "\t10.43%\r",
      "\t10.45%\r",
      "\t10.47%\r",
      "\t10.49%\r",
      "\t10.51%\r",
      "\t10.53%\r",
      "\t10.55%\r",
      "\t10.57%\r",
      "\t10.59%\r",
      "\t10.61%\r",
      "\t10.64%\r",
      "\t10.66%\r",
      "\t10.68%\r",
      "\t10.70%\r",
      "\t10.72%\r",
      "\t10.74%\r",
      "\t10.76%\r",
      "\t10.78%\r",
      "\t10.80%\r",
      "\t10.82%\r",
      "\t10.84%\r",
      "\t10.86%\r",
      "\t10.88%\r",
      "\t10.90%\r",
      "\t10.92%\r",
      "\t10.94%\r",
      "\t10.96%\r",
      "\t10.98%\r",
      "\t11.00%\r",
      "\t11.02%\r",
      "\t11.05%\r",
      "\t11.07%\r",
      "\t11.09%\r",
      "\t11.11%\r",
      "\t11.13%\r",
      "\t11.15%\r",
      "\t11.17%\r",
      "\t11.19%\r",
      "\t11.21%\r",
      "\t11.23%\r",
      "\t11.25%\r",
      "\t11.27%\r",
      "\t11.29%\r",
      "\t11.31%\r",
      "\t11.33%\r",
      "\t11.35%\r",
      "\t11.37%\r",
      "\t11.39%\r",
      "\t11.41%\r",
      "\t11.43%\r",
      "\t11.45%\r",
      "\t11.48%\r",
      "\t11.50%\r",
      "\t11.52%\r",
      "\t11.54%\r",
      "\t11.56%\r",
      "\t11.58%\r",
      "\t11.60%\r",
      "\t11.62%\r",
      "\t11.64%\r",
      "\t11.66%\r",
      "\t11.68%\r",
      "\t11.70%\r",
      "\t11.72%\r",
      "\t11.74%\r",
      "\t11.76%\r",
      "\t11.78%\r",
      "\t11.80%\r",
      "\t11.82%\r",
      "\t11.84%\r",
      "\t11.86%\r",
      "\t11.89%\r",
      "\t11.91%\r",
      "\t11.93%\r",
      "\t11.95%\r",
      "\t11.97%\r",
      "\t11.99%\r",
      "\t12.01%\r",
      "\t12.03%\r",
      "\t12.05%\r",
      "\t12.07%\r",
      "\t12.09%\r",
      "\t12.11%\r",
      "\t12.13%\r",
      "\t12.15%\r",
      "\t12.17%\r",
      "\t12.19%\r",
      "\t12.21%\r",
      "\t12.23%\r",
      "\t12.25%\r",
      "\t12.27%\r",
      "\t12.30%\r",
      "\t12.32%\r",
      "\t12.34%\r",
      "\t12.36%\r",
      "\t12.38%\r",
      "\t12.40%\r",
      "\t12.42%\r",
      "\t12.44%\r",
      "\t12.46%\r",
      "\t12.48%\r",
      "\t12.50%\r",
      "\t12.52%\r",
      "\t12.54%\r",
      "\t12.56%\r",
      "\t12.58%\r",
      "\t12.60%\r",
      "\t12.62%\r",
      "\t12.64%\r",
      "\t12.66%\r",
      "\t12.68%\r",
      "\t12.70%\r",
      "\t12.73%\r",
      "\t12.75%\r",
      "\t12.77%\r",
      "\t12.79%\r",
      "\t12.81%\r",
      "\t12.83%\r",
      "\t12.85%\r",
      "\t12.87%\r",
      "\t12.89%\r",
      "\t12.91%\r",
      "\t12.93%\r",
      "\t12.95%\r",
      "\t12.97%\r",
      "\t12.99%\r",
      "\t13.01%\r",
      "\t13.03%\r",
      "\t13.05%\r",
      "\t13.07%\r",
      "\t13.09%\r",
      "\t13.11%\r",
      "\t13.14%\r",
      "\t13.16%\r",
      "\t13.18%\r",
      "\t13.20%\r",
      "\t13.22%\r",
      "\t13.24%\r",
      "\t13.26%\r",
      "\t13.28%\r",
      "\t13.30%\r",
      "\t13.32%\r",
      "\t13.34%\r",
      "\t13.36%\r",
      "\t13.38%\r",
      "\t13.40%\r",
      "\t13.42%\r",
      "\t13.44%\r",
      "\t13.46%\r",
      "\t13.48%\r",
      "\t13.50%\r",
      "\t13.52%\r",
      "\t13.55%\r",
      "\t13.57%\r",
      "\t13.59%\r",
      "\t13.61%\r",
      "\t13.63%\r",
      "\t13.65%\r",
      "\t13.67%\r",
      "\t13.69%\r",
      "\t13.71%\r",
      "\t13.73%\r",
      "\t13.75%\r",
      "\t13.77%\r",
      "\t13.79%\r",
      "\t13.81%\r",
      "\t13.83%\r",
      "\t13.85%\r",
      "\t13.87%\r",
      "\t13.89%\r",
      "\t13.91%\r",
      "\t13.93%\r",
      "\t13.95%\r",
      "\t13.98%\r",
      "\t14.00%\r",
      "\t14.02%\r",
      "\t14.04%\r",
      "\t14.06%\r",
      "\t14.08%\r",
      "\t14.10%\r",
      "\t14.12%\r",
      "\t14.14%\r",
      "\t14.16%\r",
      "\t14.18%\r",
      "\t14.20%\r",
      "\t14.22%\r",
      "\t14.24%\r",
      "\t14.26%\r",
      "\t14.28%\r",
      "\t14.30%\r",
      "\t14.32%\r",
      "\t14.34%\r",
      "\t14.36%\r",
      "\t14.39%\r",
      "\t14.41%\r",
      "\t14.43%\r",
      "\t14.45%\r",
      "\t14.47%\r",
      "\t14.49%\r",
      "\t14.51%\r",
      "\t14.53%\r",
      "\t14.55%\r",
      "\t14.57%\r",
      "\t14.59%\r",
      "\t14.61%\r",
      "\t14.63%\r",
      "\t14.65%\r",
      "\t14.67%\r",
      "\t14.69%\r",
      "\t14.71%\r",
      "\t14.73%\r",
      "\t14.75%\r",
      "\t14.77%\r",
      "\t14.80%\r",
      "\t14.82%\r",
      "\t14.84%\r",
      "\t14.86%\r",
      "\t14.88%\r",
      "\t14.90%\r",
      "\t14.92%\r",
      "\t14.94%\r",
      "\t14.96%\r",
      "\t14.98%\r",
      "\t15.00%\r",
      "\t15.02%\r",
      "\t15.04%\r",
      "\t15.06%\r",
      "\t15.08%\r",
      "\t15.10%\r",
      "\t15.12%\r",
      "\t15.14%\r",
      "\t15.16%\r",
      "\t15.18%\r",
      "\t15.20%\r",
      "\t15.23%\r",
      "\t15.25%\r",
      "\t15.27%\r",
      "\t15.29%\r",
      "\t15.31%\r",
      "\t15.33%\r",
      "\t15.35%\r",
      "\t15.37%\r",
      "\t15.39%\r",
      "\t15.41%\r",
      "\t15.43%\r",
      "\t15.45%\r",
      "\t15.47%\r",
      "\t15.49%\r",
      "\t15.51%\r",
      "\t15.53%\r",
      "\t15.55%\r",
      "\t15.57%\r",
      "\t15.59%\r",
      "\t15.61%\r",
      "\t15.64%\r",
      "\t15.66%\r",
      "\t15.68%\r",
      "\t15.70%\r",
      "\t15.72%\r",
      "\t15.74%\r",
      "\t15.76%\r",
      "\t15.78%\r",
      "\t15.80%\r",
      "\t15.82%\r",
      "\t15.84%\r",
      "\t15.86%\r",
      "\t15.88%\r",
      "\t15.90%\r",
      "\t15.92%\r",
      "\t15.94%\r",
      "\t15.96%\r",
      "\t15.98%\r",
      "\t16.00%\r",
      "\t16.02%\r",
      "\t16.05%\r",
      "\t16.07%\r",
      "\t16.09%\r",
      "\t16.11%\r",
      "\t16.13%\r",
      "\t16.15%\r",
      "\t16.17%\r",
      "\t16.19%\r",
      "\t16.21%\r",
      "\t16.23%\r",
      "\t16.25%\r",
      "\t16.27%\r",
      "\t16.29%\r",
      "\t16.31%\r",
      "\t16.33%\r",
      "\t16.35%\r",
      "\t16.37%\r",
      "\t16.39%\r",
      "\t16.41%\r",
      "\t16.43%\r",
      "\t16.45%\r",
      "\t16.48%\r",
      "\t16.50%\r",
      "\t16.52%\r",
      "\t16.54%\r",
      "\t16.56%\r",
      "\t16.58%\r",
      "\t16.60%\r",
      "\t16.62%\r",
      "\t16.64%\r",
      "\t16.66%\r",
      "\t16.68%\r",
      "\t16.70%\r",
      "\t16.72%\r",
      "\t16.74%\r",
      "\t16.76%\r",
      "\t16.78%\r",
      "\t16.80%\r",
      "\t16.82%\r",
      "\t16.84%\r",
      "\t16.86%\r",
      "\t16.89%\r",
      "\t16.91%\r",
      "\t16.93%\r",
      "\t16.95%\r",
      "\t16.97%\r",
      "\t16.99%\r",
      "\t17.01%\r",
      "\t17.03%\r",
      "\t17.05%\r",
      "\t17.07%\r",
      "\t17.09%\r",
      "\t17.11%\r",
      "\t17.13%\r",
      "\t17.15%\r",
      "\t17.17%\r",
      "\t17.19%\r",
      "\t17.21%\r",
      "\t17.23%\r",
      "\t17.25%\r",
      "\t17.27%\r",
      "\t17.30%\r",
      "\t17.32%\r",
      "\t17.34%\r",
      "\t17.36%\r",
      "\t17.38%\r",
      "\t17.40%\r",
      "\t17.42%\r",
      "\t17.44%\r",
      "\t17.46%\r",
      "\t17.48%\r",
      "\t17.50%\r",
      "\t17.52%\r",
      "\t17.54%\r",
      "\t17.56%\r",
      "\t17.58%\r",
      "\t17.60%\r",
      "\t17.62%\r",
      "\t17.64%\r",
      "\t17.66%\r",
      "\t17.68%\r",
      "\t17.70%\r",
      "\t17.73%\r",
      "\t17.75%\r",
      "\t17.77%\r",
      "\t17.79%\r",
      "\t17.81%\r",
      "\t17.83%\r",
      "\t17.85%\r",
      "\t17.87%\r",
      "\t17.89%\r",
      "\t17.91%\r",
      "\t17.93%\r",
      "\t17.95%\r",
      "\t17.97%\r",
      "\t17.99%\r",
      "\t18.01%\r",
      "\t18.03%\r",
      "\t18.05%\r",
      "\t18.07%\r",
      "\t18.09%\r",
      "\t18.11%\r",
      "\t18.14%\r",
      "\t18.16%\r",
      "\t18.18%\r",
      "\t18.20%\r",
      "\t18.22%\r",
      "\t18.24%\r",
      "\t18.26%\r",
      "\t18.28%\r",
      "\t18.30%\r",
      "\t18.32%\r",
      "\t18.34%\r",
      "\t18.36%\r",
      "\t18.38%\r",
      "\t18.40%\r",
      "\t18.42%\r",
      "\t18.44%\r",
      "\t18.46%\r",
      "\t18.48%\r",
      "\t18.50%\r",
      "\t18.52%\r",
      "\t18.55%\r",
      "\t18.57%\r",
      "\t18.59%\r",
      "\t18.61%\r",
      "\t18.63%\r",
      "\t18.65%\r",
      "\t18.67%\r",
      "\t18.69%\r",
      "\t18.71%\r",
      "\t18.73%\r",
      "\t18.75%\r",
      "\t18.77%\r",
      "\t18.79%\r",
      "\t18.81%\r",
      "\t18.83%\r",
      "\t18.85%\r",
      "\t18.87%\r",
      "\t18.89%\r",
      "\t18.91%\r",
      "\t18.93%\r",
      "\t18.95%\r",
      "\t18.98%\r",
      "\t19.00%\r",
      "\t19.02%\r",
      "\t19.04%\r",
      "\t19.06%\r",
      "\t19.08%\r",
      "\t19.10%\r",
      "\t19.12%\r",
      "\t19.14%\r",
      "\t19.16%\r",
      "\t19.18%\r",
      "\t19.20%\r",
      "\t19.22%\r",
      "\t19.24%\r",
      "\t19.26%\r",
      "\t19.28%\r",
      "\t19.30%\r",
      "\t19.32%\r",
      "\t19.34%\r",
      "\t19.36%\r",
      "\t19.39%\r",
      "\t19.41%\r",
      "\t19.43%\r",
      "\t19.45%\r",
      "\t19.47%\r",
      "\t19.49%\r",
      "\t19.51%\r",
      "\t19.53%\r",
      "\t19.55%\r",
      "\t19.57%\r",
      "\t19.59%\r",
      "\t19.61%\r",
      "\t19.63%\r",
      "\t19.65%\r",
      "\t19.67%\r",
      "\t19.69%\r",
      "\t19.71%\r",
      "\t19.73%\r",
      "\t19.75%\r",
      "\t19.77%\r",
      "\t19.80%\r",
      "\t19.82%\r",
      "\t19.84%\r",
      "\t19.86%\r",
      "\t19.88%\r",
      "\t19.90%\r",
      "\t19.92%\r",
      "\t19.94%\r",
      "\t19.96%\r",
      "\t19.98%\r",
      "\t20.00%\r",
      "\t20.02%\r",
      "\t20.04%\r",
      "\t20.06%\r",
      "\t20.08%\r",
      "\t20.10%\r",
      "\t20.12%\r",
      "\t20.14%\r",
      "\t20.16%\r",
      "\t20.18%\r",
      "\t20.20%\r",
      "\t20.23%\r",
      "\t20.25%\r",
      "\t20.27%\r",
      "\t20.29%\r",
      "\t20.31%\r",
      "\t20.33%\r",
      "\t20.35%\r",
      "\t20.37%\r",
      "\t20.39%\r",
      "\t20.41%\r",
      "\t20.43%\r",
      "\t20.45%\r",
      "\t20.47%\r",
      "\t20.49%\r",
      "\t20.51%\r",
      "\t20.53%\r",
      "\t20.55%\r",
      "\t20.57%\r",
      "\t20.59%\r",
      "\t20.61%\r",
      "\t20.64%\r",
      "\t20.66%\r",
      "\t20.68%\r",
      "\t20.70%\r",
      "\t20.72%\r",
      "\t20.74%\r",
      "\t20.76%\r",
      "\t20.78%\r",
      "\t20.80%\r",
      "\t20.82%\r",
      "\t20.84%\r",
      "\t20.86%\r",
      "\t20.88%\r",
      "\t20.90%\r",
      "\t20.92%\r",
      "\t20.94%\r",
      "\t20.96%\r",
      "\t20.98%\r",
      "\t21.00%\r",
      "\t21.02%\r",
      "\t21.05%\r",
      "\t21.07%\r",
      "\t21.09%\r",
      "\t21.11%\r",
      "\t21.13%\r",
      "\t21.15%\r",
      "\t21.17%\r",
      "\t21.19%\r",
      "\t21.21%\r",
      "\t21.23%\r",
      "\t21.25%\r",
      "\t21.27%\r",
      "\t21.29%\r",
      "\t21.31%\r",
      "\t21.33%\r",
      "\t21.35%\r",
      "\t21.37%\r",
      "\t21.39%\r",
      "\t21.41%\r",
      "\t21.43%\r",
      "\t21.45%\r",
      "\t21.48%\r",
      "\t21.50%\r",
      "\t21.52%\r",
      "\t21.54%\r",
      "\t21.56%\r",
      "\t21.58%\r",
      "\t21.60%\r",
      "\t21.62%\r",
      "\t21.64%\r",
      "\t21.66%\r",
      "\t21.68%\r",
      "\t21.70%\r",
      "\t21.72%\r",
      "\t21.74%\r",
      "\t21.76%\r",
      "\t21.78%\r",
      "\t21.80%\r",
      "\t21.82%\r",
      "\t21.84%\r",
      "\t21.86%\r",
      "\t21.89%\r",
      "\t21.91%\r",
      "\t21.93%\r",
      "\t21.95%\r",
      "\t21.97%\r",
      "\t21.99%\r",
      "\t22.01%\r",
      "\t22.03%\r",
      "\t22.05%\r",
      "\t22.07%\r",
      "\t22.09%\r",
      "\t22.11%\r",
      "\t22.13%\r",
      "\t22.15%\r",
      "\t22.17%\r",
      "\t22.19%\r",
      "\t22.21%\r",
      "\t22.23%\r",
      "\t22.25%\r",
      "\t22.27%\r",
      "\t22.30%\r",
      "\t22.32%\r",
      "\t22.34%\r",
      "\t22.36%\r",
      "\t22.38%\r",
      "\t22.40%\r",
      "\t22.42%\r",
      "\t22.44%\r",
      "\t22.46%\r",
      "\t22.48%\r",
      "\t22.50%\r",
      "\t22.52%\r",
      "\t22.54%\r",
      "\t22.56%\r",
      "\t22.58%\r",
      "\t22.60%\r",
      "\t22.62%\r",
      "\t22.64%\r",
      "\t22.66%\r",
      "\t22.68%\r",
      "\t22.70%\r",
      "\t22.73%\r",
      "\t22.75%\r",
      "\t22.77%\r",
      "\t22.79%\r",
      "\t22.81%\r",
      "\t22.83%\r",
      "\t22.85%\r",
      "\t22.87%\r",
      "\t22.89%\r",
      "\t22.91%\r",
      "\t22.93%\r",
      "\t22.95%\r",
      "\t22.97%\r",
      "\t22.99%\r",
      "\t23.01%\r",
      "\t23.03%\r",
      "\t23.05%\r",
      "\t23.07%\r",
      "\t23.09%\r",
      "\t23.11%\r",
      "\t23.14%\r",
      "\t23.16%\r",
      "\t23.18%\r",
      "\t23.20%\r",
      "\t23.22%\r",
      "\t23.24%\r",
      "\t23.26%\r",
      "\t23.28%\r",
      "\t23.30%\r",
      "\t23.32%\r",
      "\t23.34%\r",
      "\t23.36%\r",
      "\t23.38%\r",
      "\t23.40%\r",
      "\t23.42%\r",
      "\t23.44%\r",
      "\t23.46%\r",
      "\t23.48%\r",
      "\t23.50%\r",
      "\t23.52%\r",
      "\t23.55%\r",
      "\t23.57%\r",
      "\t23.59%\r",
      "\t23.61%\r",
      "\t23.63%\r",
      "\t23.65%\r",
      "\t23.67%\r",
      "\t23.69%\r",
      "\t23.71%\r",
      "\t23.73%\r",
      "\t23.75%\r",
      "\t23.77%\r",
      "\t23.79%\r",
      "\t23.81%\r",
      "\t23.83%\r",
      "\t23.85%\r",
      "\t23.87%\r",
      "\t23.89%\r",
      "\t23.91%\r",
      "\t23.93%\r",
      "\t23.95%\r",
      "\t23.98%\r",
      "\t24.00%\r",
      "\t24.02%\r",
      "\t24.04%\r",
      "\t24.06%\r",
      "\t24.08%\r",
      "\t24.10%\r",
      "\t24.12%\r",
      "\t24.14%\r",
      "\t24.16%\r",
      "\t24.18%\r",
      "\t24.20%\r",
      "\t24.22%\r",
      "\t24.24%\r",
      "\t24.26%\r",
      "\t24.28%\r",
      "\t24.30%\r",
      "\t24.32%\r",
      "\t24.34%\r",
      "\t24.36%\r",
      "\t24.39%\r",
      "\t24.41%\r",
      "\t24.43%\r",
      "\t24.45%\r",
      "\t24.47%\r",
      "\t24.49%\r",
      "\t24.51%\r",
      "\t24.53%\r",
      "\t24.55%\r",
      "\t24.57%\r",
      "\t24.59%\r",
      "\t24.61%\r",
      "\t24.63%\r",
      "\t24.65%\r",
      "\t24.67%\r",
      "\t24.69%\r",
      "\t24.71%\r",
      "\t24.73%\r",
      "\t24.75%\r",
      "\t24.77%\r",
      "\t24.80%\r",
      "\t24.82%\r",
      "\t24.84%\r",
      "\t24.86%\r",
      "\t24.88%\r",
      "\t24.90%\r",
      "\t24.92%\r",
      "\t24.94%\r",
      "\t24.96%\r",
      "\t24.98%\r",
      "\t25.00%\r",
      "\t25.02%\r",
      "\t25.04%\r",
      "\t25.06%\r",
      "\t25.08%\r",
      "\t25.10%\r",
      "\t25.12%\r",
      "\t25.14%\r",
      "\t25.16%\r",
      "\t25.18%\r",
      "\t25.20%\r",
      "\t25.23%\r",
      "\t25.25%\r",
      "\t25.27%\r",
      "\t25.29%\r",
      "\t25.31%\r",
      "\t25.33%\r",
      "\t25.35%\r",
      "\t25.37%\r",
      "\t25.39%\r",
      "\t25.41%\r",
      "\t25.43%\r",
      "\t25.45%\r",
      "\t25.47%\r",
      "\t25.49%\r",
      "\t25.51%\r",
      "\t25.53%\r",
      "\t25.55%\r",
      "\t25.57%\r",
      "\t25.59%\r",
      "\t25.61%\r",
      "\t25.64%\r",
      "\t25.66%\r",
      "\t25.68%\r",
      "\t25.70%\r",
      "\t25.72%\r",
      "\t25.74%\r",
      "\t25.76%\r",
      "\t25.78%\r",
      "\t25.80%\r",
      "\t25.82%\r",
      "\t25.84%\r",
      "\t25.86%\r",
      "\t25.88%\r",
      "\t25.90%\r",
      "\t25.92%\r",
      "\t25.94%\r",
      "\t25.96%\r",
      "\t25.98%\r",
      "\t26.00%\r",
      "\t26.02%\r",
      "\t26.05%\r",
      "\t26.07%\r",
      "\t26.09%\r",
      "\t26.11%\r",
      "\t26.13%\r",
      "\t26.15%\r",
      "\t26.17%\r",
      "\t26.19%\r",
      "\t26.21%\r",
      "\t26.23%\r",
      "\t26.25%\r",
      "\t26.27%\r",
      "\t26.29%\r",
      "\t26.31%\r",
      "\t26.33%\r",
      "\t26.35%\r",
      "\t26.37%\r",
      "\t26.39%\r",
      "\t26.41%\r",
      "\t26.43%\r",
      "\t26.45%\r",
      "\t26.48%\r",
      "\t26.50%\r",
      "\t26.52%\r",
      "\t26.54%\r",
      "\t26.56%\r",
      "\t26.58%\r",
      "\t26.60%\r",
      "\t26.62%\r",
      "\t26.64%\r",
      "\t26.66%\r",
      "\t26.68%\r",
      "\t26.70%\r",
      "\t26.72%\r",
      "\t26.74%\r",
      "\t26.76%\r",
      "\t26.78%\r",
      "\t26.80%\r",
      "\t26.82%\r",
      "\t26.84%\r",
      "\t26.86%\r",
      "\t26.89%\r",
      "\t26.91%\r",
      "\t26.93%\r",
      "\t26.95%\r",
      "\t26.97%\r",
      "\t26.99%\r",
      "\t27.01%\r",
      "\t27.03%\r",
      "\t27.05%\r",
      "\t27.07%\r",
      "\t27.09%\r",
      "\t27.11%\r",
      "\t27.13%\r",
      "\t27.15%\r",
      "\t27.17%\r",
      "\t27.19%\r",
      "\t27.21%\r",
      "\t27.23%\r",
      "\t27.25%\r",
      "\t27.27%\r",
      "\t27.30%\r",
      "\t27.32%\r",
      "\t27.34%\r",
      "\t27.36%\r",
      "\t27.38%\r",
      "\t27.40%\r",
      "\t27.42%\r",
      "\t27.44%\r",
      "\t27.46%\r",
      "\t27.48%\r",
      "\t27.50%\r",
      "\t27.52%\r",
      "\t27.54%\r",
      "\t27.56%\r",
      "\t27.58%\r",
      "\t27.60%\r",
      "\t27.62%\r",
      "\t27.64%\r",
      "\t27.66%\r",
      "\t27.68%\r",
      "\t27.70%\r",
      "\t27.73%\r",
      "\t27.75%\r",
      "\t27.77%\r",
      "\t27.79%\r",
      "\t27.81%\r",
      "\t27.83%\r",
      "\t27.85%\r",
      "\t27.87%\r",
      "\t27.89%\r",
      "\t27.91%\r",
      "\t27.93%\r",
      "\t27.95%\r",
      "\t27.97%\r",
      "\t27.99%\r",
      "\t28.01%\r",
      "\t28.03%\r",
      "\t28.05%\r",
      "\t28.07%\r",
      "\t28.09%\r",
      "\t28.11%\r",
      "\t28.14%\r",
      "\t28.16%\r",
      "\t28.18%\r",
      "\t28.20%\r",
      "\t28.22%\r",
      "\t28.24%\r",
      "\t28.26%\r",
      "\t28.28%\r",
      "\t28.30%\r",
      "\t28.32%\r",
      "\t28.34%\r",
      "\t28.36%\r",
      "\t28.38%\r",
      "\t28.40%\r",
      "\t28.42%\r",
      "\t28.44%\r",
      "\t28.46%\r",
      "\t28.48%\r",
      "\t28.50%\r",
      "\t28.52%\r",
      "\t28.55%\r",
      "\t28.57%\r",
      "\t28.59%\r",
      "\t28.61%\r",
      "\t28.63%\r",
      "\t28.65%\r",
      "\t28.67%\r",
      "\t28.69%\r",
      "\t28.71%\r",
      "\t28.73%\r",
      "\t28.75%\r",
      "\t28.77%\r",
      "\t28.79%\r",
      "\t28.81%\r",
      "\t28.83%\r",
      "\t28.85%\r",
      "\t28.87%\r",
      "\t28.89%\r",
      "\t28.91%\r",
      "\t28.93%\r",
      "\t28.95%\r",
      "\t28.98%\r",
      "\t29.00%\r",
      "\t29.02%\r",
      "\t29.04%\r",
      "\t29.06%\r",
      "\t29.08%\r",
      "\t29.10%\r",
      "\t29.12%\r",
      "\t29.14%\r",
      "\t29.16%\r",
      "\t29.18%\r",
      "\t29.20%\r",
      "\t29.22%\r",
      "\t29.24%\r",
      "\t29.26%\r",
      "\t29.28%\r",
      "\t29.30%\r",
      "\t29.32%\r",
      "\t29.34%\r",
      "\t29.36%\r",
      "\t29.39%\r",
      "\t29.41%\r",
      "\t29.43%\r",
      "\t29.45%\r",
      "\t29.47%\r",
      "\t29.49%\r",
      "\t29.51%\r",
      "\t29.53%\r",
      "\t29.55%\r",
      "\t29.57%\r",
      "\t29.59%\r",
      "\t29.61%\r",
      "\t29.63%\r",
      "\t29.65%\r",
      "\t29.67%\r",
      "\t29.69%\r",
      "\t29.71%\r",
      "\t29.73%\r",
      "\t29.75%\r",
      "\t29.77%\r",
      "\t29.80%\r",
      "\t29.82%\r",
      "\t29.84%\r",
      "\t29.86%\r",
      "\t29.88%\r",
      "\t29.90%\r",
      "\t29.92%\r",
      "\t29.94%\r",
      "\t29.96%\r",
      "\t29.98%\r",
      "\t30.00%\r",
      "\t30.02%\r",
      "\t30.04%\r",
      "\t30.06%\r",
      "\t30.08%\r",
      "\t30.10%\r",
      "\t30.12%\r",
      "\t30.14%\r",
      "\t30.16%\r",
      "\t30.18%\r",
      "\t30.20%\r",
      "\t30.23%\r",
      "\t30.25%\r",
      "\t30.27%\r",
      "\t30.29%\r",
      "\t30.31%\r",
      "\t30.33%\r",
      "\t30.35%\r",
      "\t30.37%\r",
      "\t30.39%\r",
      "\t30.41%\r",
      "\t30.43%\r",
      "\t30.45%\r",
      "\t30.47%\r",
      "\t30.49%\r",
      "\t30.51%\r",
      "\t30.53%\r",
      "\t30.55%\r",
      "\t30.57%\r",
      "\t30.59%\r",
      "\t30.61%\r",
      "\t30.64%\r",
      "\t30.66%\r",
      "\t30.68%\r",
      "\t30.70%\r",
      "\t30.72%\r",
      "\t30.74%\r",
      "\t30.76%\r",
      "\t30.78%\r",
      "\t30.80%\r",
      "\t30.82%\r",
      "\t30.84%\r",
      "\t30.86%\r",
      "\t30.88%\r",
      "\t30.90%\r",
      "\t30.92%\r",
      "\t30.94%\r",
      "\t30.96%\r",
      "\t30.98%\r",
      "\t31.00%\r",
      "\t31.02%\r",
      "\t31.05%\r",
      "\t31.07%\r",
      "\t31.09%\r",
      "\t31.11%\r",
      "\t31.13%\r",
      "\t31.15%\r",
      "\t31.17%\r",
      "\t31.19%\r",
      "\t31.21%\r",
      "\t31.23%\r",
      "\t31.25%\r",
      "\t31.27%\r",
      "\t31.29%\r",
      "\t31.31%\r",
      "\t31.33%\r",
      "\t31.35%\r",
      "\t31.37%\r",
      "\t31.39%\r",
      "\t31.41%\r",
      "\t31.43%\r",
      "\t31.45%\r",
      "\t31.48%\r",
      "\t31.50%\r",
      "\t31.52%\r",
      "\t31.54%\r",
      "\t31.56%\r",
      "\t31.58%\r",
      "\t31.60%\r",
      "\t31.62%\r",
      "\t31.64%\r",
      "\t31.66%\r",
      "\t31.68%\r",
      "\t31.70%\r",
      "\t31.72%\r",
      "\t31.74%\r",
      "\t31.76%\r",
      "\t31.78%\r",
      "\t31.80%\r",
      "\t31.82%\r",
      "\t31.84%\r",
      "\t31.86%\r",
      "\t31.89%\r",
      "\t31.91%\r",
      "\t31.93%\r",
      "\t31.95%\r",
      "\t31.97%\r",
      "\t31.99%\r",
      "\t32.01%\r",
      "\t32.03%\r",
      "\t32.05%\r",
      "\t32.07%\r",
      "\t32.09%\r",
      "\t32.11%\r",
      "\t32.13%\r",
      "\t32.15%\r",
      "\t32.17%\r",
      "\t32.19%\r",
      "\t32.21%\r",
      "\t32.23%\r",
      "\t32.25%\r",
      "\t32.27%\r",
      "\t32.30%\r",
      "\t32.32%\r",
      "\t32.34%\r",
      "\t32.36%\r",
      "\t32.38%\r",
      "\t32.40%\r",
      "\t32.42%\r",
      "\t32.44%\r",
      "\t32.46%\r",
      "\t32.48%\r",
      "\t32.50%\r",
      "\t32.52%\r",
      "\t32.54%\r",
      "\t32.56%\r",
      "\t32.58%\r",
      "\t32.60%\r",
      "\t32.62%\r",
      "\t32.64%\r",
      "\t32.66%\r",
      "\t32.68%\r",
      "\t32.70%\r",
      "\t32.73%\r",
      "\t32.75%\r",
      "\t32.77%\r",
      "\t32.79%\r",
      "\t32.81%\r",
      "\t32.83%\r",
      "\t32.85%\r",
      "\t32.87%\r",
      "\t32.89%\r",
      "\t32.91%\r",
      "\t32.93%\r",
      "\t32.95%\r",
      "\t32.97%\r",
      "\t32.99%\r",
      "\t33.01%\r",
      "\t33.03%\r",
      "\t33.05%\r",
      "\t33.07%\r",
      "\t33.09%\r",
      "\t33.11%\r",
      "\t33.14%\r",
      "\t33.16%\r",
      "\t33.18%\r",
      "\t33.20%\r",
      "\t33.22%\r",
      "\t33.24%\r",
      "\t33.26%\r",
      "\t33.28%\r",
      "\t33.30%\r",
      "\t33.32%\r",
      "\t33.34%\r",
      "\t33.36%\r",
      "\t33.38%\r",
      "\t33.40%\r",
      "\t33.42%\r",
      "\t33.44%\r",
      "\t33.46%\r",
      "\t33.48%\r",
      "\t33.50%\r",
      "\t33.52%\r",
      "\t33.55%\r",
      "\t33.57%\r",
      "\t33.59%\r",
      "\t33.61%\r",
      "\t33.63%\r",
      "\t33.65%\r",
      "\t33.67%\r",
      "\t33.69%\r",
      "\t33.71%\r",
      "\t33.73%\r",
      "\t33.75%\r",
      "\t33.77%\r",
      "\t33.79%\r",
      "\t33.81%\r",
      "\t33.83%\r",
      "\t33.85%\r",
      "\t33.87%\r",
      "\t33.89%\r",
      "\t33.91%\r",
      "\t33.93%\r",
      "\t33.95%\r",
      "\t33.98%\r",
      "\t34.00%\r",
      "\t34.02%\r",
      "\t34.04%\r",
      "\t34.06%\r",
      "\t34.08%\r",
      "\t34.10%\r",
      "\t34.12%\r",
      "\t34.14%\r",
      "\t34.16%\r",
      "\t34.18%\r",
      "\t34.20%\r",
      "\t34.22%\r",
      "\t34.24%\r",
      "\t34.26%\r",
      "\t34.28%\r",
      "\t34.30%\r",
      "\t34.32%\r",
      "\t34.34%\r",
      "\t34.36%\r",
      "\t34.39%\r",
      "\t34.41%\r",
      "\t34.43%\r",
      "\t34.45%\r",
      "\t34.47%\r",
      "\t34.49%\r",
      "\t34.51%\r",
      "\t34.53%\r",
      "\t34.55%\r",
      "\t34.57%\r",
      "\t34.59%\r",
      "\t34.61%\r",
      "\t34.63%\r",
      "\t34.65%\r",
      "\t34.67%\r",
      "\t34.69%\r",
      "\t34.71%\r",
      "\t34.73%\r",
      "\t34.75%\r",
      "\t34.77%\r",
      "\t34.80%\r",
      "\t34.82%\r",
      "\t34.84%\r",
      "\t34.86%\r",
      "\t34.88%\r",
      "\t34.90%\r",
      "\t34.92%\r",
      "\t34.94%\r",
      "\t34.96%\r",
      "\t34.98%\r",
      "\t35.00%\r",
      "\t35.02%\r",
      "\t35.04%\r",
      "\t35.06%\r",
      "\t35.08%\r",
      "\t35.10%\r",
      "\t35.12%\r",
      "\t35.14%\r",
      "\t35.16%\r",
      "\t35.18%\r",
      "\t35.20%\r",
      "\t35.23%\r",
      "\t35.25%\r",
      "\t35.27%\r",
      "\t35.29%\r",
      "\t35.31%\r",
      "\t35.33%\r",
      "\t35.35%\r",
      "\t35.37%\r",
      "\t35.39%\r",
      "\t35.41%\r",
      "\t35.43%\r",
      "\t35.45%\r",
      "\t35.47%\r",
      "\t35.49%\r",
      "\t35.51%\r",
      "\t35.53%\r",
      "\t35.55%\r",
      "\t35.57%\r",
      "\t35.59%\r",
      "\t35.61%\r",
      "\t35.64%\r",
      "\t35.66%\r",
      "\t35.68%\r",
      "\t35.70%\r",
      "\t35.72%\r",
      "\t35.74%\r",
      "\t35.76%\r",
      "\t35.78%\r",
      "\t35.80%\r",
      "\t35.82%\r",
      "\t35.84%\r",
      "\t35.86%\r",
      "\t35.88%\r",
      "\t35.90%\r",
      "\t35.92%\r",
      "\t35.94%\r",
      "\t35.96%\r",
      "\t35.98%\r",
      "\t36.00%\r",
      "\t36.02%\r",
      "\t36.05%\r",
      "\t36.07%\r",
      "\t36.09%\r",
      "\t36.11%\r",
      "\t36.13%\r",
      "\t36.15%\r",
      "\t36.17%\r",
      "\t36.19%\r",
      "\t36.21%\r",
      "\t36.23%\r",
      "\t36.25%\r",
      "\t36.27%\r",
      "\t36.29%\r",
      "\t36.31%\r",
      "\t36.33%\r",
      "\t36.35%\r",
      "\t36.37%\r",
      "\t36.39%\r",
      "\t36.41%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t100%00%\n",
      "common_pids: 2281\n",
      "remaining_pids: 5212\n",
      "There are 5500 train, 993 valid, 1000 test samples\n",
      "generating code code adjacent matrix ...\n",
      "\t5500 / 5500\n",
      "building train codes features and labels ...\n",
      "\t5500 / 5500\n",
      "building valid codes features and labels ...\n",
      "\t993 / 993\n",
      "building test codes features and labels ...\n",
      "\t1000 / 1000\n",
      "generating train neighbors ...\n",
      "\t5500 / 5500\n",
      "generating valid neighbors ...\n",
      "\t993 / 993\n",
      "generating test neighbors ...\n",
      "\t1000 / 1000\n",
      "generating train middles ...\n",
      "\t5500 / 5500\n",
      "generating valid middles ...\n",
      "\t993 / 993\n",
      "generating test middles ...\n",
      "\t1000 / 1000\n",
      "building train heart failure labels ...\n",
      "building valid heart failure labels ...\n",
      "building test heart failure labels ...\n",
      "saving encoded data ...\n",
      "saving standard data ...\n",
      "\tsaving training data\n",
      "\tsaving valid data\n",
      "\tsaving test data\n",
      "1941 patients have heart failure out of 5500 patients\n",
      "342 patients have heart failure out of 993 patients\n",
      "373 patients have heart failure out of 1000 patients\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import _pickle as pickle\n",
    "\n",
    "from preprocess import save_sparse, save_data\n",
    "from preprocess.parse_csv import Mimic3Parser\n",
    "from preprocess.encode import encode_code\n",
    "from preprocess.build_dataset import split_patients, build_code_xy, build_heart_failure_y\n",
    "from preprocess.auxiliary import generate_code_code_adjacent, generate_neighbors, normalize_adj, divide_middle, generate_code_levels, show_heart_failure_stat\n",
    "\n",
    "# Configuration for dataset processing specific to the Mimic3 dataset\n",
    "conf = {'mimic3': {\n",
    "        'parser': Mimic3Parser,\n",
    "        'train_num': 5500,\n",
    "        'test_num': 1000,\n",
    "        'threshold': 0.01\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flag to determine if data should be loaded from previously saved files\n",
    "from_saved = True\n",
    "data_path = 'data'\n",
    "dataset = 'mimic3' \n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "raw_path = os.path.join(dataset_path, 'raw')\n",
    "\n",
    "# Check if raw data directory exists, create if not and prompt for data placement\n",
    "if not os.path.exists(raw_path):\n",
    "    os.makedirs(raw_path)\n",
    "    print('please put the CSV files in `data/%s/raw`' % dataset)\n",
    "    exit()\n",
    "\n",
    "# Path for storing parsed data\n",
    "parsed_path = os.path.join(dataset_path, 'parsed')\n",
    "\n",
    "# Load or parse data depending on `from_saved` flag\n",
    "if from_saved:\n",
    "    # Load previously saved parsed data\n",
    "    patient_admission = pickle.load(open(os.path.join(parsed_path, 'patient_admission.pkl'), 'rb'))\n",
    "    admission_codes = pickle.load(open(os.path.join(parsed_path, 'admission_codes.pkl'), 'rb'))\n",
    "else:\n",
    "    # Parse new data from raw files\n",
    "    parser = conf[dataset]['parser'](raw_path)\n",
    "    sample_num = conf[dataset].get('sample_num', None)\n",
    "    patient_admission, admission_codes = parser.parse(sample_num)\n",
    "    print('saving parsed data ...')\n",
    "    if not os.path.exists(parsed_path):\n",
    "        os.makedirs(parsed_path)\n",
    "    pickle.dump(patient_admission, open(os.path.join(parsed_path, 'patient_admission.pkl'), 'wb'))\n",
    "    pickle.dump(admission_codes, open(os.path.join(parsed_path, 'admission_codes.pkl'), 'wb'))\n",
    "\n",
    "# Calculate various statistics from the patient admissions data\n",
    "patient_num = len(patient_admission)\n",
    "max_admission_num = max([len(admissions) for admissions in patient_admission.values()])\n",
    "avg_admission_num = sum([len(admissions) for admissions in patient_admission.values()]) / patient_num\n",
    "max_visit_code_num = max([len(codes) for codes in admission_codes.values()])\n",
    "avg_visit_code_num = sum([len(codes) for codes in admission_codes.values()]) / len(admission_codes)\n",
    "print('patient num: %d' % patient_num)\n",
    "print('max admission num: %d' % max_admission_num)\n",
    "print('mean admission num: %.2f' % avg_admission_num)\n",
    "print('max code num in an admission: %d' % max_visit_code_num)\n",
    "print('mean code num in an admission: %.2f' % avg_visit_code_num)\n",
    "\n",
    "# Encode diagnosis codes and generate a code map\n",
    "print('encoding code ...')\n",
    "admission_codes_encoded, code_map = encode_code(patient_admission, admission_codes)\n",
    "code_num = len(code_map)\n",
    "print('There are %d codes' % code_num)\n",
    "\n",
    "# Generate levels for each code and save\n",
    "code_levels = generate_code_levels(data_path, code_map)\n",
    "pickle.dump({\n",
    "    'code_levels': code_levels,\n",
    "}, open(os.path.join(parsed_path, 'code_levels.pkl'), 'wb'))\n",
    "\n",
    "# Split patients into training, validation, and test sets\n",
    "train_pids, valid_pids, test_pids = split_patients(\n",
    "    patient_admission=patient_admission,\n",
    "    admission_codes=admission_codes,\n",
    "    code_map=code_map,\n",
    "    train_num=conf[dataset]['train_num'],\n",
    "    test_num=conf[dataset]['test_num']\n",
    ")\n",
    "print('There are %d train, %d valid, %d test samples' % (len(train_pids), len(valid_pids), len(test_pids)))\n",
    "code_adj = generate_code_code_adjacent(pids=train_pids, patient_admission=patient_admission,\n",
    "                                        admission_codes_encoded=admission_codes_encoded,\n",
    "                                        code_num=code_num, threshold=conf[dataset]['threshold'])\n",
    "\n",
    "# Additional data processing for training, validation, and test sets\n",
    "common_args = [patient_admission, admission_codes_encoded, max_admission_num, code_num]\n",
    "print('building train codes features and labels ...')\n",
    "(train_code_x, train_codes_y, train_visit_lens) = build_code_xy(train_pids, *common_args)\n",
    "print('building valid codes features and labels ...')\n",
    "(valid_code_x, valid_codes_y, valid_visit_lens) = build_code_xy(valid_pids, *common_args)\n",
    "print('building test codes features and labels ...')\n",
    "(test_code_x, test_codes_y, test_visit_lens) = build_code_xy(test_pids, *common_args)\n",
    "\n",
    "print('generating train neighbors ...')\n",
    "train_neighbors = generate_neighbors(train_code_x, train_visit_lens, code_adj)\n",
    "print('generating valid neighbors ...')\n",
    "valid_neighbors = generate_neighbors(valid_code_x, valid_visit_lens, code_adj)\n",
    "print('generating test neighbors ...')\n",
    "test_neighbors = generate_neighbors(test_code_x, test_visit_lens, code_adj)\n",
    "\n",
    "print('generating train middles ...')\n",
    "train_divided = divide_middle(train_code_x, train_neighbors, train_visit_lens)\n",
    "print('generating valid middles ...')\n",
    "valid_divided = divide_middle(valid_code_x, valid_neighbors, valid_visit_lens)\n",
    "print('generating test middles ...')\n",
    "test_divided = divide_middle(test_code_x, test_neighbors, test_visit_lens)\n",
    "\n",
    "print('building train heart failure labels ...')\n",
    "train_hf_y = build_heart_failure_y('428', train_codes_y, code_map)\n",
    "print('building valid heart failure labels ...')\n",
    "valid_hf_y = build_heart_failure_y('428', valid_codes_y, code_map)\n",
    "print('building test heart failure labels ...')\n",
    "test_hf_y = build_heart_failure_y('428', test_codes_y, code_map)\n",
    "\n",
    "# Save all processed data in appropriate paths\n",
    "encoded_path = os.path.join(dataset_path, 'encoded')\n",
    "if not os.path.exists(encoded_path):\n",
    "    os.makedirs(encoded_path)\n",
    "print('saving encoded data ...')\n",
    "pickle.dump(patient_admission, open(os.path.join(encoded_path, 'patient_admission.pkl'), 'wb'))\n",
    "pickle.dump(admission_codes_encoded, open(os.path.join(encoded_path, 'codes_encoded.pkl'), 'wb'))\n",
    "pickle.dump(code_map, open(os.path.join(encoded_path, 'code_map.pkl'), 'wb'))\n",
    "pickle.dump({\n",
    "    'train_pids': train_pids,\n",
    "    'valid_pids': valid_pids,\n",
    "    'test_pids': test_pids\n",
    "}, open(os.path.join(encoded_path, 'pids.pkl'), 'wb'))\n",
    "\n",
    "print('saving standard data ...')\n",
    "standard_path = os.path.join(dataset_path, 'standard')\n",
    "train_path = os.path.join(standard_path, 'train')\n",
    "valid_path = os.path.join(standard_path, 'valid')\n",
    "test_path = os.path.join(standard_path, 'test')\n",
    "if not os.path.exists(standard_path):\n",
    "    os.makedirs(standard_path)\n",
    "if not os.path.exists(train_path):\n",
    "    os.makedirs(train_path)\n",
    "    os.makedirs(valid_path)\n",
    "    os.makedirs(test_path)\n",
    "\n",
    "print('\\tsaving training data')\n",
    "save_data(train_path, train_code_x, train_visit_lens, train_codes_y, train_hf_y, train_divided, train_neighbors)\n",
    "print('\\tsaving valid data')\n",
    "save_data(valid_path, valid_code_x, valid_visit_lens, valid_codes_y, valid_hf_y, valid_divided, valid_neighbors)\n",
    "print('\\tsaving test data')\n",
    "save_data(test_path, test_code_x, test_visit_lens, test_codes_y, test_hf_y, test_divided, test_neighbors)\n",
    "\n",
    "code_adj = normalize_adj(code_adj)\n",
    "save_sparse(os.path.join(standard_path, 'code_adj'), code_adj)\n",
    "\n",
    "# Show statistics for heart failure\n",
    "\n",
    "show_heart_failure_stat('428', train_codes_y, code_map)\n",
    "show_heart_failure_stat('428', valid_codes_y, code_map)\n",
    "show_heart_failure_stat('428', test_codes_y, code_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Chet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SingleHeadAttentionLayer**: Implements a typical single-head attention mechanism, commonly found in architectures like the Transformer model. It uses linear transformations for the queries, keys, and values, computes scaled dot-product attention scores, and then computes a weighted sum of the values based on these scores.\n",
    "  \n",
    "- **DotProductAttention**: A simpler form of attention where a context vector learns to identify relevant features from the input data. It projects transformed inputs onto this context vector, calculates attention scores, and then weights the input data according to these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # This will print True if PyTorch is using CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define a SingleHeadAttentionLayer class that extends nn.Module.\n",
    "class SingleHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, attention_size):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        self.attention_size = attention_size  # Store the attention size\n",
    "        \n",
    "        # Define Linear transformations for query, key, and value vectors\n",
    "        self.dense_q = nn.Linear(query_size, attention_size)  # Transforms input query to the attention space\n",
    "        self.dense_k = nn.Linear(key_size, attention_size)    # Transforms input key to the attention space\n",
    "        self.dense_v = nn.Linear(query_size, value_size)      # Transforms input value, no change in dimension\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # Apply linear transformations\n",
    "        query = self.dense_q(q)  # Transform query vector\n",
    "        key = self.dense_k(k)    # Transform key vector\n",
    "        value = self.dense_v(v)  # Transform value vector\n",
    "        \n",
    "        # Compute the attention scores\n",
    "        # Scaled dot product attention mechanism\n",
    "        g = torch.div(torch.matmul(query, key.T), math.sqrt(self.attention_size))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        score = torch.softmax(g, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of values based on the attention scores\n",
    "        output = torch.sum(torch.unsqueeze(score, dim=-1) * value, dim=-2)\n",
    "        return output\n",
    "\n",
    "# Define a DotProductAttention class that also extends nn.Module.\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, value_size, attention_size):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        self.attention_size = attention_size  # Store the attention size\n",
    "        # Initialize a context vector as a learnable parameter\n",
    "        self.context = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(attention_size, 1)))\n",
    "        self.dense = nn.Linear(value_size, attention_size)  # Transforms input value to the attention space\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transform input x to attention space\n",
    "        t = self.dense(x)\n",
    "        \n",
    "        # Compute unnormalized attention scores by projecting 't' onto 'context'\n",
    "        vu = torch.matmul(t, self.context).squeeze()\n",
    "        \n",
    "        # Apply softmax to get normalized attention weights\n",
    "        score = torch.softmax(vu, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of the original inputs based on the attention weights\n",
    "        output = torch.sum(x * torch.unsqueeze(score, dim=-1), dim=-2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmbeddingLayer\n",
    "- Purpose: The EmbeddingLayer is responsible for transforming discrete code identifiers into dense vector representations. This is a fundamental step in many neural network models that deal with categorical data, enabling the model to capture and leverage the relationships and patterns inherent in the data more effectively.\n",
    "- Functionality: Three Types of Embeddings: It initializes three types of embeddings for the codes: center (c_embeddings), neighbor (n_embeddings), and a general use embedding (u_embeddings). Each of these embeddings serves different roles in the graph-based computations that follow, potentially representing different aspects or features of the data.\n",
    "- Parameter Initialization: The embeddings are initialized using Xavier uniform distribution, which is a common practice for initializing weights in neural networks in a way that aims to maintain the variance of activations across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# EmbeddingLayer: Handles the embedding of codes into vectors.\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, code_num, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        # Number of unique codes\n",
    "        self.code_num = code_num\n",
    "        # Embedding parameters initialized using Xavier uniform distribution\n",
    "        # c_embeddings for center node embeddings\n",
    "        self.c_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
    "        # n_embeddings for neighbor node embeddings\n",
    "        self.n_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
    "        # u_embeddings for other uses, potentially for graph-level embeddings\n",
    "        self.u_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, graph_size)))\n",
    "\n",
    "    def forward(self):\n",
    "        # Return all embeddings as outputs\n",
    "        return self.c_embeddings, self.n_embeddings, self.u_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphLayer\n",
    "- Purpose: The GraphLayer utilizes the embeddings provided by the EmbeddingLayer to perform computations reflecting the relationships and interactions encoded in an adjacency matrix. This layer is crucial for models that incorporate graph theory to process data structured as graphs (e.g., social networks, molecule structures).\n",
    "- Functionality:\n",
    "  - Embedding Interaction: It computes new embeddings based on interactions between center and neighbor nodes using the adjacency matrix. This involves matrix multiplication operations that simulate the propagation of information through the graph.\n",
    "  - Transformation and Non-linearity: After computing the initial interactions, the embeddings are transformed through a linear layer and passed through a non-linear activation function (LeakyReLU), enhancing the model's ability to capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphLayer: Processes embeddings via graph structure using adjacency matrix.\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, adj, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        self.adj = torch.tensor(adj, dtype=torch.float32)  # Convert numpy array to tensor\n",
    "        self.dense = nn.Linear(code_size, graph_size)  # Dense layer for transforming embeddings\n",
    "        self.activation = nn.LeakyReLU()  # Activation function to introduce non-linearity\n",
    "\n",
    "    def forward(self, code_x, neighbor, c_embeddings, n_embeddings):\n",
    "        # Apply unsqueeze to expand dimensions for matrix operations\n",
    "        center_codes = torch.unsqueeze(code_x, dim=-1)\n",
    "        neighbor_codes = torch.unsqueeze(neighbor, dim=-1)\n",
    "\n",
    "        # Compute embeddings based on input codes and their neighbors\n",
    "        center_embeddings = center_codes * c_embeddings\n",
    "        neighbor_embeddings = neighbor_codes * n_embeddings\n",
    "        # Multiply embeddings by adjacency matrix to propagate through the graph\n",
    "        cc_embeddings = center_codes * torch.matmul(self.adj, center_embeddings)\n",
    "        cn_embeddings = center_codes * torch.matmul(self.adj, neighbor_embeddings)\n",
    "        nn_embeddings = neighbor_codes * torch.matmul(self.adj, neighbor_embeddings)\n",
    "        nc_embeddings = neighbor_codes * torch.matmul(self.adj, center_embeddings)\n",
    "\n",
    "        # Combine embeddings and pass through dense layer with activation\n",
    "        co_embeddings = self.activation(self.dense(center_embeddings + cc_embeddings + cn_embeddings))\n",
    "        no_embeddings = self.activation(self.dense(neighbor_embeddings + nn_embeddings + nc_embeddings))\n",
    "        return co_embeddings, no_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransitionLayer\n",
    "- Purpose: The TransitionLayer manages dynamic changes in the graph structure, reflecting transitions over time or between states. This layer is essential for temporal or dynamic graph models where the state of the graph changes in a way that is significant for the task (e.g., temporal changes in a patient's health records).\n",
    "- Functionality:\n",
    "  - State Update with GRU: It uses a GRUCell for updating the hidden states based on the current embeddings. GRUs are effective in managing sequences where the current output is dependent on previous states, making them suitable for time-series data or any data with temporal dynamics.\n",
    "  - Attention Mechanism: Incorporates a single-head attention mechanism to selectively focus on important features from the embeddings. This is particularly useful in complex scenarios where not all parts of the input are equally relevant to the output.\n",
    "  - Dynamic Handling of Divisions: It processes divided inputs (perhaps representing different categories or types of inputs) and manages transitions based on these divisions, reflecting complex internal dynamics within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransitionLayer: Manages state transitions in graph sequences using GRU and attention.\n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, code_num, graph_size, hidden_size, t_attention_size, t_output_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(input_size=graph_size, hidden_size=hidden_size)  # GRU cell for state transitions\n",
    "        # Attention layer for processing graph-level information\n",
    "        self.single_head_attention = SingleHeadAttentionLayer(graph_size, graph_size, t_output_size, t_attention_size)\n",
    "        self.activation = nn.Tanh()  # Tanh activation for smooth non-linearity\n",
    "\n",
    "        self.code_num = code_num  # Total number of codes\n",
    "        self.hidden_size = hidden_size  # Dimension of hidden state\n",
    "\n",
    "    def forward(self, t, co_embeddings, divided, no_embeddings, unrelated_embeddings, hidden_state=None):\n",
    "        # Process middle states based on input divisions\n",
    "        m1, m2, m3 = divided[:, 0], divided[:, 1], divided[:, 2]\n",
    "        # Find indices where division values are positive\n",
    "        m1_index = torch.where(m1 > 0)[0]\n",
    "        m2_index = torch.where(m2 > 0)[0]\n",
    "        m3_index = torch.where(m3 > 0)[0]\n",
    "        # Initialize new hidden state for all codes\n",
    "        h_new = torch.zeros((self.code_num, self.hidden_size), dtype=co_embeddings.dtype).to(co_embeddings.device)\n",
    "        output_m1 = 0\n",
    "        output_m23 = 0\n",
    "        # Compute new state for m1 divisions\n",
    "        if len(m1_index) > 0:\n",
    "            m1_embedding = co_embeddings[m1_index]\n",
    "            h = hidden_state[m1_index] if hidden_state is not None else None\n",
    "            h_m1 = self.gru(m1_embedding, h)\n",
    "            h_new[m1_index] = h_m1\n",
    "            output_m1, _ = torch.max(h_m1, dim=-2)\n",
    "        # Compute new state for m2 and m3 divisions if t > 0\n",
    "        if t > 0 and len(m2_index) + len(m3_index) > 0:\n",
    "            # Combine embeddings for m2 and m3 indices for attention processing\n",
    "            q = torch.vstack([no_embeddings[m2_index], unrelated_embeddings[m3_index]])\n",
    "            v = torch.vstack([co_embeddings[m2_index], co_embeddings[m3_index]])\n",
    "            \n",
    "            # Process combined embeddings through the attention layer\n",
    "            h_m23 = self.activation(self.single_head_attention(q, q, v))\n",
    "            \n",
    "            # Update the hidden states for m2 and m3 indices based on attention outputs\n",
    "            h_new[m2_index] = h_m23[:len(m2_index)]\n",
    "            h_new[m3_index] = h_m23[len(m2_index):]\n",
    "            \n",
    "            # Determine the maximum output across m2 and m3 for use in the final output\n",
    "            output_m23, _ = torch.max(h_m23, dim=-2)\n",
    "        \n",
    "        # Determine the final output based on available indices\n",
    "        if len(m1_index) == 0:\n",
    "            output = output_m23\n",
    "        elif len(m2_index) + len(m3_index) == 0:\n",
    "            output = output_m1\n",
    "        else:\n",
    "            # If both m1 and m2/m3 indices have outputs, take the maximum across both\n",
    "            output, _ = torch.max(torch.vstack([output_m1, output_m23]), dim=-2)\n",
    "\n",
    "        # Return the final aggregated output and the updated hidden states\n",
    "        return output, h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Component**          | **Description**                                                                                                                                                                       |\n",
    "|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Classifier**         | - **Purpose**: Serves as the final classification layer in the model pipeline.<br>- **Components**: Consists of a linear layer, optional activation function, and a dropout layer.<br>- **Functionality**: Takes the output from the preceding layers, applies dropout for regularization, transforms it through a linear operation, and finally applies an activation function if provided. This setup is typical for neural network classifiers to make final predictions. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: A simple neural network module for classification tasks.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0., activation=None):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)  # Linear transformation to the output size\n",
    "        self.activation = activation  # Optional activation function\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Dropout layer to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.dropout(x)  # Apply dropout to the input\n",
    "        output = self.linear(output)  # Apply linear transformation\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)  # Apply activation function if provided\n",
    "        return output  # Return the final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Component**          | **Description**                                                                                                                                                                       |\n",
    "|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Chet**              | - **Purpose**: Integrates various specialized layers to process graph-structured data and sequences.<br>- **Components**: Includes `EmbeddingLayer`, `GraphLayer`, `TransitionLayer`, `DotProductAttention`, and the `Classifier`.<br>- **Functionality**: Orchestrates the flow of data through multiple layers designed to handle embeddings, graph interactions, transitions in dynamic states, and finally classification. |\n",
    "| **forward (Chet)**    | - **Purpose**: Defines how data passes through the model during the forward pass.<br>- **Functionality**: <ul><li>Obtains embeddings from `EmbeddingLayer`.</li><li>Processes these through `GraphLayer` for each sequence element.</li><li>Uses `TransitionLayer` to manage transitions and update states based on dynamic graph elements.</li><li>Applies `DotProductAttention` to aggregate sequence data effectively.</li><li>Feeds the aggregated output into `Classifier` for final predictions.</li></ul>|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Main model integrating various components for processing graphs.\n",
    "class Chet(nn.Module):\n",
    "    def __init__(self, code_num, code_size, adj, graph_size, hidden_size,\n",
    "                 t_attention_size, t_output_size, output_size, dropout_rate, activation):\n",
    "        super().__init__()\n",
    "        # Initialize embedding, graph, and transition layers\n",
    "        self.embedding_layer = EmbeddingLayer(code_num, code_size, graph_size)\n",
    "        self.graph_layer = GraphLayer(adj, code_size, graph_size)\n",
    "        self.transition_layer = TransitionLayer(code_num, graph_size, hidden_size, t_attention_size, t_output_size)\n",
    "        self.attention = DotProductAttention(hidden_size, 32)  # Dot product attention mechanism\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate, activation)  # Classifier component\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        # Generate embeddings from the embedding layer\n",
    "        embeddings = self.embedding_layer()\n",
    "        c_embeddings, n_embeddings, u_embeddings = embeddings\n",
    "        output = []\n",
    "        # Process each sequence in the batch\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            no_embeddings_i_prev = None  # Store previous neighbor embeddings\n",
    "            output_i = []\n",
    "            h_t = None  # Initialize hidden state\n",
    "            # Iterate over time steps within a sequence\n",
    "            for t, (c_it, d_it, n_it, len_it) in enumerate(zip(code_x_i, divided_i, neighbor_i, range(len_i))):\n",
    "                # Process current time step using the graph layer\n",
    "                co_embeddings, no_embeddings = self.graph_layer(c_it, n_it, c_embeddings, n_embeddings)\n",
    "                # Transition layer updates based on current and previous states\n",
    "                output_it, h_t = self.transition_layer(t, co_embeddings, d_it, no_embeddings_i_prev, u_embeddings, h_t)\n",
    "                no_embeddings_i_prev = no_embeddings  # Update previous embeddings\n",
    "                output_i.append(output_it)  # Collect outputs for each time step\n",
    "            # Apply attention to the sequence of outputs\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)  # Collect final outputs for all sequences\n",
    "\n",
    "        output = torch.vstack(output)  # Stack all sequence outputs\n",
    "        output = self.classifier(output)  # Classify the aggregated outputs\n",
    "        return output  # Return the final model output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Component** | **Sub-Component**          | **Purpose**                                          | **Functionality**                                                                                                                                                           |\n",
    "|---------------|----------------------------|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Chet_TF**   | **Embedding Layer**        | To initialize code embeddings                        | Converts `code_num`, `code_size`, and `graph_size` into dense vector representations of the input codes, facilitating easier and more effective processing by neural networks. |\n",
    "|               | **Graph Layer**            | To process graph-based data                          | Uses the adjacency matrix (`adj`) along with embeddings to process relationships between different codes in a graph-structured data setup.                                    |\n",
    "|               | **Linear Transition Layer**| To transform graph layer outputs                     | Maps the output from the `GraphLayer` (of size `graph_size`) to a higher dimensional space (`hidden_size`) using a linear transformation, followed by a `tanh` activation to introduce non-linearity. **This layer is to replace the original Transition layer.**                            |\n",
    "|               | **Dot Product Attention**  | To emphasize important features                      | Applies a dot product attention mechanism on the sequence of outputs, focusing on significant features that are crucial for the prediction task.                              |\n",
    "|               | **Classifier**             | To classify the aggregated outputs                   | Uses the output from the attention mechanism to make final classifications. Includes dropout for regularization and uses the specified activation function for the final output layer.                                                 |\n",
    "| **Forward**   | **Data Processing Loop**   | To handle sequence processing per batch              | Iterates over each sequence in the batch, processing each time step's data through the graph layer, followed by the transition layer, and collects outputs for attention processing.                                                  |\n",
    "|               | **Attention and Classification** | To aggregate and classify outputs                 | Aggregates the transformed outputs using attention, and then classifies these aggregated outputs using the classifier. The final output is stacked and returned as the model's prediction for the batch.                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Chet_TF(nn.Module):\n",
    "    def __init__(self, code_num, code_size, adj, graph_size, hidden_size,\n",
    "                 output_size, dropout_rate, activation, t_attention_size, t_output_size):\n",
    "        super().__init__()\n",
    "        # Initialize embedding, graph, and the linear transition layers\n",
    "        self.embedding_layer = EmbeddingLayer(code_num, code_size, graph_size)\n",
    "        self.graph_layer = GraphLayer(adj, code_size, graph_size)\n",
    "        self.linear_transition = nn.Linear(graph_size, hidden_size)  # Linear layer to replace transition function\n",
    "        self.transition_dropout = nn.Dropout(0.5)  # Dropout layer after linear transition\n",
    "        self.tanh = nn.Tanh()  # Tanh activation function\n",
    "        self.attention = DotProductAttention(hidden_size, 32)  # Dot product attention mechanism\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate, activation)  # Classifier component\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        # Generate embeddings from the embedding layer\n",
    "        embeddings = self.embedding_layer()\n",
    "        c_embeddings, n_embeddings, u_embeddings = embeddings\n",
    "        output = []\n",
    "        # Process each sequence in the batch\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            output_i = []\n",
    "            # Iterate over time steps within a sequence\n",
    "            for c_it, n_it in zip(code_x_i, neighbor_i):\n",
    "                # Process current time step using the graph layer\n",
    "                co_embeddings, _ = self.graph_layer(c_it, n_it, c_embeddings, n_embeddings)\n",
    "                # Apply the linear layer and then tanh activation\n",
    "                transformed_output = self.tanh(self.linear_transition(co_embeddings))\n",
    "                output_i.append(transformed_output)  # Collect outputs for each time step\n",
    "            # Apply attention to the sequence of outputs\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)  # Collect final outputs for all sequences\n",
    "        output = torch.vstack(output)  # Stack all sequence outputs\n",
    "        output = self.classifier(output)  # Classify the aggregated outputs\n",
    "        return output  # Return the final model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step\n",
    "- Historical Data Feature: Implements a feature extraction method historical_hot to transform code data based on historical visits, which could be a critical feature depending on the task.\n",
    "- Dynamic Model Handling: Allows flexibility in model choice and parameters, facilitating easy experiments with different configurations.\n",
    "- Robust Training and Validation Mechanics: Incorporates modern training enhancements like dynamic learning rate adjustments and early stopping based on validation performance to optimize training outcomes.\n",
    "- Resource Management: Carefully manages device resources, ensuring that all operations are performed on the designated computing device (GPU/CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils import load_adj, EHRDataset, format_time, MultiStepLRScheduler\n",
    "from metrics import evaluate_codes, evaluate_hf\n",
    "from models.model import Model\n",
    "\n",
    "\n",
    "def historical_hot(code_x, code_num, lens):\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, (x, l) in enumerate(zip(code_x, lens)):\n",
    "        result[i] = x[l - 1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size):\n",
    "    seed = 1000\n",
    "    dataset = 'mimic3'  # 'mimic3' or 'eicu'\n",
    "    task = 'h'  # 'm' or 'h'\n",
    "    model_name = model_name\n",
    "    use_cuda = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "\n",
    "    code_size = code_size\n",
    "    graph_size = graph_size\n",
    "    hidden_size = hidden_size  # rnn hidden size\n",
    "    t_attention_size = t_attention_size\n",
    "    t_output_size = hidden_size\n",
    "    batch_size = batch_size\n",
    "    epochs = epochs\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    dataset_path = os.path.join('data', dataset, 'standard')\n",
    "    train_path = os.path.join(dataset_path, 'train')\n",
    "    valid_path = os.path.join(dataset_path, 'valid')\n",
    "    test_path = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    code_adj = load_adj(dataset_path, device=device)\n",
    "    code_num = len(code_adj)\n",
    "    print('loading train data ...')\n",
    "    train_data = EHRDataset(train_path, label=task, batch_size=batch_size, shuffle=True, device=device)\n",
    "    print('loading valid data ...')\n",
    "    valid_data = EHRDataset(valid_path, label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "    print('loading test data ...')\n",
    "    test_data = EHRDataset(test_path, label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "\n",
    "    test_historical = historical_hot(valid_data.code_x, code_num, valid_data.visit_lens)\n",
    "\n",
    "    task_conf = {\n",
    "        'h': {\n",
    "            'dropout': 0.0,\n",
    "            'output_size': 1,\n",
    "            'evaluate_fn': evaluate_hf,\n",
    "            'lr': {\n",
    "                'init_lr': 0.01,\n",
    "                'milestones': [2, 3, 20],\n",
    "                'lrs': [1e-3, 1e-4, 1e-5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    model_select = {\n",
    "        'Chet': Chet,\n",
    "        'Chet_TF': Chet_TF,\n",
    "    }\n",
    "\n",
    "    output_size = task_conf[task]['output_size']\n",
    "    activation = torch.nn.Sigmoid()\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    evaluate_fn = task_conf[task]['evaluate_fn']\n",
    "    dropout_rate = task_conf[task]['dropout']\n",
    "\n",
    "    param_path = os.path.join('data', 'params', dataset, task, model_name)\n",
    "    if not os.path.exists(param_path):\n",
    "        os.makedirs(param_path)\n",
    "\n",
    "    model = model_select[model_name](code_num=code_num, code_size=code_size,\n",
    "                    adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                    t_output_size=t_output_size,\n",
    "                    output_size=output_size, dropout_rate=dropout_rate, activation=activation).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    print(\"Epochs:\", epochs)\n",
    "    print(\"Initial LR:\", task_conf[task]['lr']['init_lr'])\n",
    "    print(\"Milestones:\", task_conf[task]['lr']['milestones'])\n",
    "    print(\"Learning Rates:\", task_conf[task]['lr']['lrs'])\n",
    "\n",
    "    scheduler = MultiStepLRScheduler(optimizer, epochs, task_conf[task]['lr']['init_lr'],\n",
    "                                        task_conf[task]['lr']['milestones'], task_conf[task]['lr']['lrs'])\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "\n",
    "    best_score = float('-inf')  # Initialize to a large number\n",
    "    best_epoch = -1  # Track the epoch at which the best model was found\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %d / %d:' % (epoch + 1, epochs))\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_num = 0\n",
    "        steps = len(train_data)\n",
    "        st = time.time()\n",
    "        scheduler.step()\n",
    "        for step in range(len(train_data)):\n",
    "            optimizer.zero_grad()\n",
    "            code_x, visit_lens, divided, y, neighbors = train_data[step]\n",
    "            output = model(code_x, divided, neighbors, visit_lens).squeeze()\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * output_size * len(code_x)\n",
    "            total_num += len(code_x)\n",
    "\n",
    "            end_time = time.time()\n",
    "            remaining_time = format_time((end_time - st) / (step + 1) * (steps - step - 1))\n",
    "            print('\\r    Step %d / %d, remaining time: %s, loss: %.4f'\n",
    "                    % (step + 1, steps, remaining_time, total_loss / total_num), end='')\n",
    "        train_data.on_epoch_end()\n",
    "        et = time.time()\n",
    "        time_cost = format_time(et - st)\n",
    "        print('\\r    Step %d / %d, time cost: %s, loss: %.4f' % (steps, steps, time_cost, total_loss / total_num))\n",
    "\n",
    "        # Evaluate the model on validation data\n",
    "        # Set model to evaluation mode for validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "            current_score = evaluate_fn(model, valid_data, loss_fn, output_size, test_historical)\n",
    "            print('\\r    composite score is: %.4f' % current_score)\n",
    "\n",
    "        # Save the model only if the validation loss improved\n",
    "        # Check if current score is better; if so, save the model\n",
    "        if current_score > best_score:\n",
    "            print(f'Improved from {best_score:.4f} to {current_score:.4f}. Saving model...')\n",
    "            best_score = current_score\n",
    "            best_epoch = epoch\n",
    "            model_save_path = os.path.join(param_path, 'best_model.pt')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            no_improvement_count = 0  # Reset counter\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f'No improvement. Count: {no_improvement_count}/{20}')\n",
    "\n",
    "        if no_improvement_count >= 20:\n",
    "            print(f'Stopping early after {epoch+1} epochs.')\n",
    "            break  # Exit the loop if no improvement in the last 20 epochs\n",
    "            \n",
    "\n",
    "        \n",
    "    # print out the best epoch and its performance after training is complete\n",
    "    print(f'Best performing model was at epoch {best_epoch + 1} with a score of {best_score:.4f}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the Full Chet model if it's available and store.\n",
    "- Train the Full Chet model if it's not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded from data/params/mimic3/h/Chet/best_model.pt\n",
      "Chet(\n",
      "  (embedding_layer): EmbeddingLayer()\n",
      "  (graph_layer): GraphLayer(\n",
      "    (dense): Linear(in_features=48, out_features=32, bias=True)\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (transition_layer): TransitionLayer(\n",
      "    (gru): GRUCell(32, 100)\n",
      "    (single_head_attention): SingleHeadAttentionLayer(\n",
      "      (dense_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (dense_k): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (dense_v): Linear(in_features=32, out_features=100, bias=True)\n",
      "    )\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (attention): DotProductAttention(\n",
      "    (dense): Linear(in_features=100, out_features=32, bias=True)\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (activation): Sigmoid()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters: 675185\n",
      "Layer: embedding_layer.c_embeddings | Size: torch.Size([4880, 48]) | Total parameters: 234240\n",
      "Layer: embedding_layer.n_embeddings | Size: torch.Size([4880, 48]) | Total parameters: 234240\n",
      "Layer: embedding_layer.u_embeddings | Size: torch.Size([4880, 32]) | Total parameters: 156160\n",
      "Layer: graph_layer.dense.weight | Size: torch.Size([32, 48]) | Total parameters: 1536\n",
      "Layer: graph_layer.dense.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: transition_layer.gru.weight_ih | Size: torch.Size([300, 32]) | Total parameters: 9600\n",
      "Layer: transition_layer.gru.weight_hh | Size: torch.Size([300, 100]) | Total parameters: 30000\n",
      "Layer: transition_layer.gru.bias_ih | Size: torch.Size([300]) | Total parameters: 300\n",
      "Layer: transition_layer.gru.bias_hh | Size: torch.Size([300]) | Total parameters: 300\n",
      "Layer: transition_layer.single_head_attention.dense_q.weight | Size: torch.Size([32, 32]) | Total parameters: 1024\n",
      "Layer: transition_layer.single_head_attention.dense_q.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: transition_layer.single_head_attention.dense_k.weight | Size: torch.Size([32, 32]) | Total parameters: 1024\n",
      "Layer: transition_layer.single_head_attention.dense_k.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: transition_layer.single_head_attention.dense_v.weight | Size: torch.Size([100, 32]) | Total parameters: 3200\n",
      "Layer: transition_layer.single_head_attention.dense_v.bias | Size: torch.Size([100]) | Total parameters: 100\n",
      "Layer: attention.context | Size: torch.Size([32, 1]) | Total parameters: 32\n",
      "Layer: attention.dense.weight | Size: torch.Size([32, 100]) | Total parameters: 3200\n",
      "Layer: attention.dense.bias | Size: torch.Size([32]) | Total parameters: 32\n",
      "Layer: classifier.linear.weight | Size: torch.Size([1, 100]) | Total parameters: 100\n",
      "Layer: classifier.linear.bias | Size: torch.Size([1]) | Total parameters: 1\n"
     ]
    }
   ],
   "source": [
    "from models.model import Model\n",
    "\n",
    "model_name = 'Chet'\n",
    "model_path = os.path.join('data', 'params', 'mimic3', 'h', model_name, 'best_model.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "code_size = 48\n",
    "graph_size = 32\n",
    "hidden_size = 100  # rnn hidden size\n",
    "t_attention_size = 32\n",
    "t_output_size = hidden_size\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "output_size = 1\n",
    "activation = torch.nn.Sigmoid()\n",
    "dropout_rate = 0.0\n",
    "\n",
    "#model_Chet = train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size)\n",
    "model_Chet = Chet(code_num=code_num, code_size=code_size,\n",
    "                  adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                  t_output_size=t_output_size,\n",
    "                  output_size=output_size, dropout_rate=dropout_rate, activation=activation).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Load the model if it exists\n",
    "    print(\"Loading model...\")\n",
    "    model_Chet.load_state_dict(torch.load(model_path))\n",
    "    model_Chet.to(device)\n",
    "    print(f'Model loaded from {model_path}')\n",
    "else:\n",
    "    # Train the model if the file does not exist\n",
    "    # Assuming train_data, optimizer, and loss_fn are defined elsewhere\n",
    "    print('Model is not available, starting training...')\n",
    "    model_Chet = train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size)\n",
    "    \n",
    "    # After training, save the model to the specified path\n",
    "    print(f'Saving model to {model_path}')\n",
    "    torch.save(model_Chet.state_dict(), model_path)\n",
    "    \n",
    "\n",
    "print(model_Chet)\n",
    "total_params = sum(p.numel() for p in model_Chet.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "for name, param in model_Chet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()} | Total parameters: {param.numel()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the Chet_TF model if it's available and store.\n",
    "- Train the Chet_TF model if it's not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is not available, starting training...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "Epochs: 100\n",
      "Initial LR: 0.01\n",
      "Milestones: [2, 3, 20]\n",
      "Learning Rates: [0.001, 0.0001, 1e-05]\n",
      "milestones:  [1, 2, 3, 20, 101]\n",
      "632873\n",
      "Epoch 1 / 100:\n",
      "    Step 172 / 172, time cost: 12m46.5s, loss: 0.52633357\n",
      "    Evaluation: loss: 0.4769 --- auc: 0.8239 --- f1_score: 0.7180\n",
      "    composite score is: 0.7334\n",
      "Improved from -inf to 0.7334. Saving model...\n",
      "Epoch 2 / 100:\n",
      "    Step 172 / 172, time cost: 12m46.7s, loss: 0.47533101\n",
      "    Evaluation: loss: 0.4715 --- auc: 0.8228 --- f1_score: 0.7180\n",
      "    composite score is: 0.7341\n",
      "Improved from 0.7334 to 0.7341. Saving model...\n",
      "Epoch 3 / 100:\n",
      "    Step 172 / 172, time cost: 12m47.3s, loss: 0.47177685\n",
      "    Evaluation: loss: 0.4702 --- auc: 0.8225 --- f1_score: 0.7180\n",
      "    composite score is: 0.7342\n",
      "Improved from 0.7341 to 0.7342. Saving model...\n",
      "Epoch 4 / 100:\n",
      "    Step 172 / 172, time cost: 12m47.8s, loss: 0.47155141\n",
      "    Evaluation: loss: 0.4704 --- auc: 0.8226 --- f1_score: 0.7180\n",
      "    composite score is: 0.7342\n",
      "Improved from 0.7342 to 0.7342. Saving model...\n",
      "Epoch 5 / 100:\n",
      "    Step 172 / 172, time cost: 12m48.3s, loss: 0.47144123\n",
      "    Evaluation: loss: 0.4696 --- auc: 0.8215 --- f1_score: 0.7180\n",
      "    composite score is: 0.7340\n",
      "No improvement. Count: 1/20\n",
      "Epoch 6 / 100:\n",
      "    Step 172 / 172, time cost: 12m48.4s, loss: 0.47122564\n",
      "    Evaluation: loss: 0.4696 --- auc: 0.8249 --- f1_score: 0.7180\n",
      "    composite score is: 0.7351\n",
      "Improved from 0.7347 to 0.7351. Saving model...\n",
      "Epoch 8 / 100:\n",
      "    Step 172 / 172, time cost: 12m48.3s, loss: 0.47100547\n",
      "    Evaluation: loss: 0.4694 --- auc: 0.8252 --- f1_score: 0.7180\n",
      "    composite score is: 0.7352\n",
      "Improved from 0.7351 to 0.7352. Saving model...\n",
      "Epoch 9 / 100:\n",
      "    Step 172 / 172, time cost: 12m48.3s, loss: 0.47088965\n",
      "    Evaluation: loss: 0.4692 --- auc: 0.8264 --- f1_score: 0.7180\n",
      "    composite score is: 0.7356\n",
      "Improved from 0.7352 to 0.7356. Saving model...\n",
      "Epoch 10 / 100:\n",
      "    Step 172 / 172, time cost: 12m48.0s, loss: 0.47066304\n",
      "    Evaluation: loss: 0.4688 --- auc: 0.8245 --- f1_score: 0.7180\n",
      "    composite score is: 0.7351\n",
      "No improvement. Count: 1/20\n",
      "Epoch 11 / 100:\n",
      "    Step 119 / 172, remaining time: 3m56.8s, loss: 0.4705"
     ]
    }
   ],
   "source": [
    "model_name = 'Chet_TF'\n",
    "model_path = os.path.join('data', 'params', 'mimic3', 'h', model_name, 'best_model.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "code_size = 48\n",
    "graph_size = 32\n",
    "hidden_size = 100  # rnn hidden size\n",
    "t_attention_size = 32\n",
    "t_output_size = hidden_size\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "output_size = 1\n",
    "activation = torch.nn.Sigmoid()\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model_tf = Chet_TF(code_num=code_num, code_size=code_size,\n",
    "                    adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                    t_output_size=t_output_size,\n",
    "                    output_size=output_size, dropout_rate=dropout_rate, activation=activation).to(device)\n",
    "\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Load the model if it exists\n",
    "    model_tf.load_state_dict(torch.load(model_path))\n",
    "    model_tf.to(device)\n",
    "    print(f'Model loaded from {model_path}')\n",
    "else:\n",
    "    # Train the model if the file does not exist\n",
    "    # Assuming train_data, optimizer, and loss_fn are defined elsewhere\n",
    "    print('Model is not available, starting training...')\n",
    "    model_tf = train(model_name, dropout_rate, epochs, code_size, graph_size, hidden_size, t_attention_size, t_output_size, batch_size)\n",
    "\n",
    "    # After training, save the model to the specified path\n",
    "    print(f'Saving model to {model_path}')\n",
    "    torch.save(model_tf.state_dict(), model_path)\n",
    "\n",
    "print(model_tf)\n",
    "total_params = sum(p.numel() for p in model_tf.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "for name, param in model_tf.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()} | Total parameters: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "## Results\n",
    "\n",
    "Evaluate the two models Chet Full Model (Chet) and Chet Without TF (Chet_TF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_29966/2064905599.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_loss_chet_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score_chet_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_Chet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_historical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_loss_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_historical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/CS598-DLH-Chet/metrics.py\u001b[0m in \u001b[0;36mevaluate_hf\u001b[0;34m(model, dataset, loss_fn, output_size, historical)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mcode_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisit_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisit_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# output is a scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_29966/293247404.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, code_x, divided, neighbors, lens)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_it\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_x_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivided_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;31m# Process current time step using the graph layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mco_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# Transition layer updates based on current and previous states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0moutput_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mco_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_embeddings_i_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_29966/332981788.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, code_x, neighbor, c_embeddings, n_embeddings)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mneighbor_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbor_codes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Multiply embeddings by adjacency matrix to propagate through the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcc_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter_codes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcn_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter_codes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnn_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbor_codes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_hf\n",
    "\n",
    "dataset_path = os.path.join('data', dataset, 'standard')\n",
    "test_path = os.path.join(dataset_path, 'test')\n",
    "test_data = EHRDataset(test_path, label='h', batch_size=32, shuffle=False, device=device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "output_size = 1\n",
    "code_adj = load_adj(dataset_path, device=device)\n",
    "code_num = len(code_adj)\n",
    "test_historical = historical_hot(test_data.code_x, code_num, test_data.visit_lens)\n",
    "\n",
    "\n",
    "test_loss_chet_tf, f1_score_chet_tf = evaluate_hf(model_Chet, test_data, loss_fn, output_size, test_historical)\n",
    "test_loss_tf, f1_score_tf = evaluate_hf(model_tf, test_data, loss_fn, output_size, test_historical)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Chet model test loss: {test_loss_chet_tf:.4f}, F1 score: {f1_score_chet_tf:.4f}')\n",
    "print(f'Chet_TF model test loss: {test_loss_tf:.4f}, F1 score: {f1_score_tf:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "### Model comparison\n",
    "\n",
    "Based on the metrics obtained from testing, the Full Chet model demonstrated superior performance, exhibiting a lower loss value compared to the variant without the transition layer. This suggests that the inclusion of the transition layer in the Full Chet model may contribute significantly to its ability to more effectively minimize errors in predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "## Discussion\n",
    "\n",
    "- **Based on the outcomes observed thus far:**\n",
    "\n",
    "1. The Full Chet model outperforms the variant lacking the transition layer, corroborating the findings reported in the original paper [1].\n",
    "2. The reproducibility of the results from the original paper [1] is affirmed due to the accessibility of both the data and the model.\n",
    "3. Constructing three levels of graph embeddings from global diagnostic codes presented significant challenges. Nevertheless, the availability of the original author's code facilitated understanding and reproduction of these methods.\n",
    "\n",
    "- **Areas for Improvement:**\n",
    "1. Currently, the dataset in use comprises a demo set with only 10 patients. To augment the dataset, this data was replicated ten times, resulting in 100 patient records.\n",
    "2. The intention is to employ the actual MIMIC-III dataset for retraining the model once it becomes accessible.\n",
    "3. The control model, a CNN-based approach known as Deepr [2], has not yet been implemented. Its integration is planned for subsequent phases to enable a more comprehensive analysis of results.\n",
    "4. There is a plan to refactor the code to leverage more widely-used packages such as DataLoader, enhancing its efficiency and readability.\n",
    "5. Efforts are underway to refine the code further, contributing to the pyHealth project, which aims to provide robust health informatics solutions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] Lu, Chang, Tian Han, and Yue Ning. \"Context-aware health event prediction via transition functions on dynamic disease graphs.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 4, pp. 4567-4574. 2022. <br>\n",
    "[2] Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe and Svetha Venkatesh, \" Deepr : A Convolutional Net for Medical Records,\" in IEEE Journal of Biomedical and Health Informatics, vol. 21, no. 1, pp. 22-30, Jan. 2017.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5",
     "timestamp": 1709153069464
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
